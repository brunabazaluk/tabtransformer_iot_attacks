{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6lrMTxY1nCvD"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow --upgrade\n",
    "#!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_Nm8d-39nDCr",
    "outputId": "ebfbd14a-3319-4539-e039-141eaf159614"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:01:39.854747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 00:01:39.873176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-02 00:01:39.873191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-02 00:01:39.873669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 00:01:39.876787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 00:01:40.214907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import data as tf_data\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uk5kQtrVnDFN"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# root_dir = \"/content/gdrive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qeZMWoyRnDHm"
   },
   "outputs": [],
   "source": [
    "# !unzip gdrive/MyDrive/packet_features.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWxegZ-AnDKK",
    "outputId": "73ac51c7-80a6-48db-b5ce-1fe803093ed6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134309/509904704.py:3: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mqtt_bruteforce = pd.read_csv('packet_features/mqtt_bruteforce.csv')\n",
      "/tmp/ipykernel_134309/509904704.py:4: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sparta = pd.read_csv('packet_features/sparta.csv')\n",
      "/tmp/ipykernel_134309/509904704.py:5: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv('packet_features/normal.csv')\n"
     ]
    }
   ],
   "source": [
    "scana = pd.read_csv('packet_features/scan_A.csv')\n",
    "scansu = pd.read_csv('packet_features/scan_sU.csv')\n",
    "mqtt_bruteforce = pd.read_csv('packet_features/mqtt_bruteforce.csv')\n",
    "sparta = pd.read_csv('packet_features/sparta.csv')\n",
    "normal = pd.read_csv('packet_features/normal.csv')\n",
    "\n",
    "#data = pd.concat([scana, scansu,mqtt_bruteforce,sparta,normal],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q0Pkvbb4Dkqm",
    "outputId": "4d8b53ff-2e42-49d7-8258-1121ae43a300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "0    70768\n",
       "1    40624\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scana['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ae0eA0jCDkqp",
    "outputId": "4d7f55c8-1f1d-4eeb-d1d4-77f329635e81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "0    210819\n",
       "1     22436\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scansu['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aquLeUpBDkqr",
    "outputId": "d3961296-3fed-402c-8815-001e3ed7f85f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "1            19103574\n",
       "0              917206\n",
       "1              625369\n",
       "0               29971\n",
       "is_attack          20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparta['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0Wn1IwJDkqv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oT75qfm_Dkqw",
    "outputId": "2becaa4b-22d8-4f7e-8dbf-04817da43364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "1            9686086\n",
       "1             327056\n",
       "0              31550\n",
       "0                614\n",
       "is_attack         10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqtt_bruteforce['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9gVCDGquD60A"
   },
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "\n",
    "    data = data.drop(columns=[\"timestamp\",\"src_ip\",\"dst_ip\", \"src_port\",\"dst_port\"])\n",
    "    data['protocol'] = data['protocol'].astype(str)\n",
    "    data['is_attack'] = data['is_attack'].astype(str)\n",
    "\n",
    "    non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in data.columns:\n",
    "        if col not in ['protocol', 'is_attack']:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "    for column in data:\n",
    "        if column not in ['protocol', 'is_attack']:\n",
    "            clean_col = data[column].dropna()\n",
    "            mean = clean_col.mean()\n",
    "            data[column].fillna(mean, inplace=True)\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    data['is_attack'].replace('0', 'no', inplace=True)\n",
    "    data['is_attack'].replace('1', 'yes', inplace=True)\n",
    "\n",
    "    data.loc[data['is_attack'] == 'is_attack', 'is_attack'] = 'yes'\n",
    "\n",
    "    #BALANCE DATASET\n",
    "\n",
    "    aux = data.groupby('is_attack')\n",
    "    data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
    "    print('quantity of yes: ', data['is_attack'].value_counts()[1])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X0jGHWF5ExO9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134309/2533644030.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
      "/tmp/ipykernel_134309/2533644030.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
      "/tmp/ipykernel_134309/2533644030.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of yes: ', data['is_attack'].value_counts()[1])\n",
      "/tmp/ipykernel_134309/2533644030.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity of no:  40624\n",
      "quantity of yes:  40624\n",
      "quantity of no:  22436\n",
      "quantity of yes:  22436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134309/2533644030.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
      "/tmp/ipykernel_134309/2533644030.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
      "/tmp/ipykernel_134309/2533644030.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of yes: ', data['is_attack'].value_counts()[1])\n",
      "/tmp/ipykernel_134309/2533644030.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
      "/tmp/ipykernel_134309/2533644030.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
      "/tmp/ipykernel_134309/2533644030.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of yes: ', data['is_attack'].value_counts()[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity of no:  32164\n",
      "quantity of yes:  32164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134309/2533644030.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
      "/tmp/ipykernel_134309/2533644030.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
      "/tmp/ipykernel_134309/2533644030.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of yes: ', data['is_attack'].value_counts()[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity of no:  947177\n",
      "quantity of yes:  947177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134309/2533644030.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_134309/2533644030.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
      "/tmp/ipykernel_134309/2533644030.py:32: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
      "/tmp/ipykernel_134309/2533644030.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print('quantity of yes: ', data['is_attack'].value_counts()[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity of no:  1\n",
      "quantity of yes:  1\n"
     ]
    }
   ],
   "source": [
    "scana = pre_processing(scana)\n",
    "scansu =  pre_processing(scansu)\n",
    "mqtt_bruteforce = pre_processing(mqtt_bruteforce)\n",
    "sparta = pre_processing(sparta)\n",
    "normal = pre_processing(normal)\n",
    "\n",
    "data = pd.concat([scana, scansu,mqtt_bruteforce,normal,sparta],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jeN8faF4EuJB"
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "X, y = data.drop(columns=[\"protocol\",\"is_attack\"]).values, data['is_attack'].values\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "#print('X shape: ', X.shape)\n",
    "#print('X columns: ', len(data.columns))\n",
    "#print('X_new shape: ', X_new.shape)\n",
    "#print('feature importances: ', importances)\n",
    "\n",
    "selected_indexes = [i+1 for i in range(len(importances)) if importances[i] >= 0.1]\n",
    "selected_indexes = selected_indexes+[0,len(data.columns)-1]\n",
    "\n",
    "not_sel_col = [data.columns[i] for i in range(len(data.columns)) if i not in selected_indexes]\n",
    "sel_col = [data.columns[i] for i in range(len(data.columns)) if i in selected_indexes]\n",
    "\n",
    "data = data.drop(columns=not_sel_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d2Sx-LiiqBxI"
   },
   "outputs": [],
   "source": [
    "#data = pd.concat([scana, scansu],axis=0)\n",
    "#data = pd.concat([scana, scansu,mqtt_bruteforce,normal],axis=0)\n",
    "\n",
    "#data = data.drop(columns=[\"timestamp\",\"src_ip\",\"dst_ip\", \"src_port\",\"dst_port\"])\n",
    "#data['protocol'] = data['protocol'].astype(str)\n",
    "#data['is_attack'] = data['is_attack'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YnFr4wB0R16W",
    "outputId": "11024737-5504-4df5-cf6e-b0cfaba40b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "no     1042402\n",
       "yes    1042402\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBpAtoWNQVS3",
    "outputId": "cd20630e-d5a8-4364-d7dd-659181b3ce8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "no     1042402\n",
       "yes    1042402\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GUdlv66QPZBI"
   },
   "outputs": [],
   "source": [
    "non_numeric_columns = data.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI8XJxDcQ3k-",
    "outputId": "48444e55-753f-47b9-a003-4d2a5ccac54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['protocol', 'is_attack'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8rQdXvYlSCQV"
   },
   "outputs": [],
   "source": [
    "#for col in data.columns:\n",
    "#    if col not in ['protocol', 'is_attack']:\n",
    "#      data[col] = pd.to_numeric(data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BQM_y5RtOZA",
    "outputId": "74ee217c-a7fa-4d8c-c39b-962030655510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "protocol               object\n",
       "ttl                   float64\n",
       "tcp_flag_push         float64\n",
       "mqtt_messagetype      float64\n",
       "mqtt_messagelength    float64\n",
       "is_attack              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ChkQXZuEOqMx",
    "outputId": "0145a7f5-00fa-48d7-dcc1-8c563a46ed4c"
   },
   "outputs": [],
   "source": [
    "#for column in data:\n",
    "#    if column not in ['protocol', 'is_attack']:\n",
    "#        clean_col = data[column].dropna()\n",
    "#        mean = clean_col.mean()\n",
    "#        data[column].fillna(mean, inplace=True)\n",
    "#\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "#\n",
    "#data['is_attack'].replace('0', 'no', inplace=True)\n",
    "#data['is_attack'].replace('1', 'yes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ENKR7UsCnDMk"
   },
   "outputs": [],
   "source": [
    "#ADD WEIGHTS - 1 FOR ALL SAMPLES\n",
    "data['fnlwgt'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12y9XwPHTK0U",
    "outputId": "e65d1f71-55d0-4283-e7e4-3b24ec0929e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "no     1042402\n",
       "yes    1042402\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2uuULEn1wmdt"
   },
   "outputs": [],
   "source": [
    "# Replace 'is_attack' with 'yes' only in rows where the value of 'is_attack' column is 'is_attack'\n",
    "#data.loc[data['is_attack'] == 'is_attack', 'is_attack'] = 'yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iU-HJfzyxw_c",
    "outputId": "ca5c4e13-1adb-4a93-aea1-82e28ce4d8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "no     1042402\n",
       "yes    1042402\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN9cnu-InDPH",
    "outputId": "052cd9c1-e396-4e10-a449-d23d39fbfe5f"
   },
   "outputs": [],
   "source": [
    "##BALANCE DATASET\n",
    "#\n",
    "#aux = data.groupby('is_attack')\n",
    "#data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
    "#\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "#print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
    "#print('quantity of yes: ', data['is_attack'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WZMjON4LnDRu"
   },
   "outputs": [],
   "source": [
    "data_train = data.loc[:int(0.8*len(data))]\n",
    "data_test = data.loc[int(0.8*len(data)):(int(0.8*len(data))+int(0.1*len(data)))]\n",
    "data_valid = data.loc[(int(0.8*len(data))+int(0.1*len(data))):]\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "valid_data_file = \"val_data.csv\"\n",
    "\n",
    "data_train.to_csv(train_data_file, index=False, header=False)\n",
    "data_test.to_csv(test_data_file, index=False, header=False)\n",
    "data_valid.to_csv(valid_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "opxG8tU_Dkq9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "73OcOd9wYJUg",
    "outputId": "30cfb420-5275-43b8-c6fe-86d8d3b889dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 3273\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:03:53.680745: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.698221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.698314: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.699281: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.699346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.699389: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.732126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.732216: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.732273: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-02 00:03:53.732312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 859 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:03:55.065135: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fabdc017740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-02 00:03:55.065151: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-02-02 00:03:55.122538: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-02 00:03:55.407906: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-02 00:03:56.261480: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2024-02-02 00:03:56.261491: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-02-02 00:03:56.261519: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-02-02 00:03:56.263066: W external/local_xla/xla/service/gpu/buffer_comparator.cc:1054] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas\n",
      "This message will only be logged once.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706821437.705476  136053 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6294/Unknown \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.7002 - auc: 0.8040 - loss: 0.4740 - precision: 0.6735 - recall: 0.7731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:04:09.375874: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "/home/irciss/anaconda3/envs/py39/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-02-02 00:04:09.414720: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.414739: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.414794: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.414923: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.416153: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.416169: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.416305: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-02 00:04:09.416473: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.7002 - auc: 0.8040 - loss: 0.4740 - precision: 0.6735 - recall: 0.7731 - val_accuracy: 0.7536 - val_auc: 0.8591 - val_loss: 0.3844 - val_precision: 0.6707 - val_recall: 0.9977\n",
      "Epoch 2/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7421 - auc: 0.8508 - loss: 0.3927 - precision: 0.6868 - recall: 0.8907 - val_accuracy: 0.7536 - val_auc: 0.8611 - val_loss: 0.3836 - val_precision: 0.6707 - val_recall: 0.9977\n",
      "Epoch 3/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7442 - auc: 0.8514 - loss: 0.3902 - precision: 0.6827 - recall: 0.9131 - val_accuracy: 0.7537 - val_auc: 0.8589 - val_loss: 0.3830 - val_precision: 0.6707 - val_recall: 0.9977\n",
      "Epoch 4/20\n",
      "\u001b[1m6280/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7455 - auc: 0.8520 - loss: 0.3892 - precision: 0.6815 - recall: 0.9224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:04:34.558254: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7455 - auc: 0.8520 - loss: 0.3892 - precision: 0.6815 - recall: 0.9224 - val_accuracy: 0.7538 - val_auc: 0.8622 - val_loss: 0.3823 - val_precision: 0.6708 - val_recall: 0.9977\n",
      "Epoch 5/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7458 - auc: 0.8519 - loss: 0.3886 - precision: 0.6803 - recall: 0.9281 - val_accuracy: 0.7538 - val_auc: 0.8591 - val_loss: 0.3821 - val_precision: 0.6708 - val_recall: 0.9977\n",
      "Epoch 6/20\n",
      "\u001b[1m 122/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7467 - auc: 0.8515 - loss: 0.3922 - precision: 0.6753 - recall: 0.9561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:04:43.247306: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:04:43.247323: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7465 - auc: 0.8524 - loss: 0.3878 - precision: 0.6797 - recall: 0.9329 - val_accuracy: 0.7539 - val_auc: 0.8614 - val_loss: 0.3823 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 7/20\n",
      "\u001b[1m 134/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7478 - auc: 0.8527 - loss: 0.3917 - precision: 0.6771 - recall: 0.9532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:04:51.422851: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7471 - auc: 0.8526 - loss: 0.3875 - precision: 0.6799 - recall: 0.9345 - val_accuracy: 0.7539 - val_auc: 0.8602 - val_loss: 0.3827 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 8/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7469 - auc: 0.8524 - loss: 0.3873 - precision: 0.6790 - recall: 0.9371 - val_accuracy: 0.7539 - val_auc: 0.8591 - val_loss: 0.3822 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 9/20\n",
      "\u001b[1m6278/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7471 - auc: 0.8527 - loss: 0.3870 - precision: 0.6788 - recall: 0.9388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:15.116405: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:05:15.116421: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:05:15.116425: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7471 - auc: 0.8527 - loss: 0.3870 - precision: 0.6788 - recall: 0.9388 - val_accuracy: 0.7539 - val_auc: 0.8605 - val_loss: 0.3818 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 10/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7473 - auc: 0.8529 - loss: 0.3869 - precision: 0.6786 - recall: 0.9400 - val_accuracy: 0.7539 - val_auc: 0.8598 - val_loss: 0.3825 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 11/20\n",
      "\u001b[1m6263/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7477 - auc: 0.8532 - loss: 0.3867 - precision: 0.6789 - recall: 0.9406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:31.453722: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7477 - auc: 0.8532 - loss: 0.3867 - precision: 0.6789 - recall: 0.9406 - val_accuracy: 0.7539 - val_auc: 0.8595 - val_loss: 0.3815 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 12/20\n",
      "\u001b[1m6293/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7473 - auc: 0.8528 - loss: 0.3868 - precision: 0.6781 - recall: 0.9421"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:39.620286: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7473 - auc: 0.8528 - loss: 0.3868 - precision: 0.6781 - recall: 0.9421 - val_accuracy: 0.7539 - val_auc: 0.8615 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 13/20\n",
      "\u001b[1m 119/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7478 - auc: 0.8521 - loss: 0.3910 - precision: 0.6755 - recall: 0.9599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:40.155615: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7479 - auc: 0.8536 - loss: 0.3867 - precision: 0.6789 - recall: 0.9410 - val_accuracy: 0.7539 - val_auc: 0.8591 - val_loss: 0.3818 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 14/20\n",
      "\u001b[1m 123/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7472 - auc: 0.8515 - loss: 0.3915 - precision: 0.6748 - recall: 0.9600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:48.315291: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:05:48.315320: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:05:48.315327: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:05:48.315332: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:05:48.315337: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:05:48.315343: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:05:48.315351: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6289/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7479 - auc: 0.8538 - loss: 0.3865 - precision: 0.6791 - recall: 0.9406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:56.022958: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7479 - auc: 0.8538 - loss: 0.3865 - precision: 0.6791 - recall: 0.9406 - val_accuracy: 0.7539 - val_auc: 0.8595 - val_loss: 0.3816 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 15/20\n",
      "\u001b[1m 127/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7455 - auc: 0.8537 - loss: 0.3908 - precision: 0.6743 - recall: 0.9557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:05:56.563880: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:05:56.563898: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7477 - auc: 0.8539 - loss: 0.3864 - precision: 0.6792 - recall: 0.9394 - val_accuracy: 0.7539 - val_auc: 0.8605 - val_loss: 0.3817 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 16/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7480 - auc: 0.8539 - loss: 0.3865 - precision: 0.6792 - recall: 0.9408 - val_accuracy: 0.7539 - val_auc: 0.8634 - val_loss: 0.3812 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 17/20\n",
      "\u001b[1m6278/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7480 - auc: 0.8541 - loss: 0.3864 - precision: 0.6793 - recall: 0.9399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:06:20.505914: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:06:20.505931: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:06:20.505933: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:06:20.505935: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:06:20.505937: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:06:20.505941: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:06:20.505945: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7480 - auc: 0.8541 - loss: 0.3864 - precision: 0.6793 - recall: 0.9399 - val_accuracy: 0.7539 - val_auc: 0.8614 - val_loss: 0.3821 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 18/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7480 - auc: 0.8539 - loss: 0.3863 - precision: 0.6787 - recall: 0.9423 - val_accuracy: 0.7539 - val_auc: 0.8614 - val_loss: 0.3815 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Epoch 19/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7480 - auc: 0.8543 - loss: 0.3861 - precision: 0.6791 - recall: 0.9408 - val_accuracy: 0.7011 - val_auc: 0.8611 - val_loss: 0.3861 - val_precision: 0.9930 - val_recall: 0.4057\n",
      "Epoch 20/20\n",
      "\u001b[1m 126/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7462 - auc: 0.8502 - loss: 0.3914 - precision: 0.6732 - recall: 0.9629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:06:37.591601: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6263/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7481 - auc: 0.8544 - loss: 0.3861 - precision: 0.6790 - recall: 0.9415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:06:45.338294: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:06:45.338317: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:06:45.338324: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:06:45.338329: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:06:45.338334: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:06:45.338340: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:06:45.338350: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7481 - auc: 0.8544 - loss: 0.3861 - precision: 0.6790 - recall: 0.9415 - val_accuracy: 0.7539 - val_auc: 0.8615 - val_loss: 0.3813 - val_precision: 0.6709 - val_recall: 0.9977\n",
      "Training finished in 171.90 seconds\n",
      "Model training finished\n",
      "Validation accuracy: 75.39%\n",
      "Inference time: 0.51 seconds\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.51      0.67    104121\n",
      "           1       0.67      1.00      0.80    104360\n",
      "\n",
      "    accuracy                           0.75    208481\n",
      "   macro avg       0.83      0.75      0.74    208481\n",
      "weighted avg       0.83      0.75      0.74    208481\n",
      "\n",
      "AUC ROC Score: 0.7535874219707134\n",
      "Total model weights: 21049\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Start training the model...\n",
      "Epoch 1/20\n",
      "   6294/Unknown \u001b[1m41s\u001b[0m 4ms/step - accuracy: 0.7270 - auc: 0.8414 - loss: 0.4195 - precision: 0.6969 - recall: 0.8039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:07:28.337380: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:07:28.337403: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:07:28.337410: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:07:28.337415: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:07:28.337420: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:07:28.337426: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:07:28.337437: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n",
      "/home/irciss/anaconda3/envs/py39/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.7270 - auc: 0.8414 - loss: 0.4195 - precision: 0.6969 - recall: 0.8039 - val_accuracy: 0.6992 - val_auc: 0.8442 - val_loss: 0.4016 - val_precision: 0.9842 - val_recall: 0.4057 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7366 - auc: 0.8467 - loss: 0.3967 - precision: 0.6963 - recall: 0.8577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:07:30.606273: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6292/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7408 - auc: 0.8508 - loss: 0.3936 - precision: 0.6892 - recall: 0.8776"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:07:49.057695: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:07:49.057716: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:07:49.057723: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:07:49.057729: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:07:49.057734: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:07:49.057740: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:07:49.057750: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7408 - auc: 0.8508 - loss: 0.3936 - precision: 0.6892 - recall: 0.8776 - val_accuracy: 0.7406 - val_auc: 0.8624 - val_loss: 0.3862 - val_precision: 0.7640 - val_recall: 0.6970 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7433 - auc: 0.8537 - loss: 0.3892 - precision: 0.6947 - recall: 0.8842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:07:49.824121: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:07:49.824143: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:07:49.824146: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:07:49.824148: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:07:49.824150: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:07:49.824152: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:07:49.824157: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7441 - auc: 0.8530 - loss: 0.3899 - precision: 0.6871 - recall: 0.8971 - val_accuracy: 0.7533 - val_auc: 0.8520 - val_loss: 0.3842 - val_precision: 0.6704 - val_recall: 0.9977 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m  51/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7440 - auc: 0.8572 - loss: 0.3871 - precision: 0.6984 - recall: 0.8794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:08:09.426121: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6287/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7452 - auc: 0.8537 - loss: 0.3888 - precision: 0.6863 - recall: 0.9039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:08:28.629009: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:08:28.629031: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:08:28.629038: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:08:28.629044: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:08:28.629049: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:08:28.629055: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:08:28.629066: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7452 - auc: 0.8537 - loss: 0.3888 - precision: 0.6863 - recall: 0.9039 - val_accuracy: 0.7410 - val_auc: 0.8593 - val_loss: 0.3840 - val_precision: 0.7648 - val_recall: 0.6970 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m  50/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7469 - auc: 0.8529 - loss: 0.3871 - precision: 0.6976 - recall: 0.8892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:08:29.385341: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:08:29.385360: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6281/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7453 - auc: 0.8542 - loss: 0.3881 - precision: 0.6855 - recall: 0.9071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:08:47.782839: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:08:47.782863: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:08:47.782870: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:08:47.782876: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:08:47.782881: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:08:47.782887: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:08:47.782897: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7453 - auc: 0.8542 - loss: 0.3881 - precision: 0.6855 - recall: 0.9071 - val_accuracy: 0.7534 - val_auc: 0.8593 - val_loss: 0.3847 - val_precision: 0.6704 - val_recall: 0.9977 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m6286/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7461 - auc: 0.8545 - loss: 0.3878 - precision: 0.6854 - recall: 0.9105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:09:06.930733: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:09:06.930756: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7461 - auc: 0.8545 - loss: 0.3878 - precision: 0.6854 - recall: 0.9105 - val_accuracy: 0.7135 - val_auc: 0.8455 - val_loss: 0.4164 - val_precision: 0.7171 - val_recall: 0.7064 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m  54/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7407 - auc: 0.8565 - loss: 0.3869 - precision: 0.7026 - recall: 0.8524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:09:07.688994: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7473 - auc: 0.8555 - loss: 0.3869 - precision: 0.6819 - recall: 0.9278 - val_accuracy: 0.7538 - val_auc: 0.8615 - val_loss: 0.3815 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7482 - auc: 0.8587 - loss: 0.3845 - precision: 0.6880 - recall: 0.9246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:09:26.376013: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:09:26.376032: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:09:26.376035: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:09:26.376040: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7483 - auc: 0.8559 - loss: 0.3863 - precision: 0.6807 - recall: 0.9359 - val_accuracy: 0.7543 - val_auc: 0.8595 - val_loss: 0.3828 - val_precision: 0.6756 - val_recall: 0.9794 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m  58/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.7442 - auc: 0.8533 - loss: 0.3861 - precision: 0.6832 - recall: 0.9268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:09:45.264878: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7481 - auc: 0.8556 - loss: 0.3865 - precision: 0.6804 - recall: 0.9364 - val_accuracy: 0.7538 - val_auc: 0.8633 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m  55/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7488 - auc: 0.8572 - loss: 0.3846 - precision: 0.6913 - recall: 0.9165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:10:04.233170: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6292/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7482 - auc: 0.8564 - loss: 0.3862 - precision: 0.6795 - recall: 0.9399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:10:22.436826: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:10:22.436848: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:10:22.436855: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:10:22.436860: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:10:22.436865: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:10:22.436871: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:10:22.436883: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7482 - auc: 0.8564 - loss: 0.3862 - precision: 0.6795 - recall: 0.9399 - val_accuracy: 0.7538 - val_auc: 0.8596 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7483 - auc: 0.8557 - loss: 0.3863 - precision: 0.6796 - recall: 0.9403 - val_accuracy: 0.7538 - val_auc: 0.8602 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m  50/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7489 - auc: 0.8559 - loss: 0.3849 - precision: 0.6901 - recall: 0.9209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:10:42.599606: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:10:42.599627: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:10:42.599630: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:10:42.599632: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:10:42.599634: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:10:42.599637: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:10:42.599642: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7486 - auc: 0.8561 - loss: 0.3862 - precision: 0.6796 - recall: 0.9413 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m  58/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.7492 - auc: 0.8567 - loss: 0.3854 - precision: 0.6872 - recall: 0.9305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:11:01.702832: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7484 - auc: 0.8560 - loss: 0.3861 - precision: 0.6792 - recall: 0.9420 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7525 - auc: 0.8573 - loss: 0.3857 - precision: 0.6940 - recall: 0.9198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:11:20.905688: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6287/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7488 - auc: 0.8559 - loss: 0.3862 - precision: 0.6793 - recall: 0.9435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:11:39.947780: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:11:39.947805: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:11:39.947811: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:11:39.947816: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:11:39.947821: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:11:39.947826: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:11:39.947831: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7488 - auc: 0.8559 - loss: 0.3862 - precision: 0.6793 - recall: 0.9435 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m6283/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7490 - auc: 0.8560 - loss: 0.3861 - precision: 0.6795 - recall: 0.9432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:11:58.657516: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:11:58.657538: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:11:58.657545: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:11:58.657551: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:11:58.657556: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:11:58.657562: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:11:58.657573: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7490 - auc: 0.8560 - loss: 0.3861 - precision: 0.6795 - recall: 0.9432 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7455 - auc: 0.8522 - loss: 0.3851 - precision: 0.6870 - recall: 0.9188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:11:59.408107: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7489 - auc: 0.8563 - loss: 0.3860 - precision: 0.6793 - recall: 0.9435 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7489 - auc: 0.8559 - loss: 0.3863 - precision: 0.6795 - recall: 0.9429 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m  53/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7453 - auc: 0.8547 - loss: 0.3849 - precision: 0.6851 - recall: 0.9248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:12:38.213431: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15880083212177553425\n",
      "2024-02-02 00:12:38.213457: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12605384355916460293\n",
      "2024-02-02 00:12:38.213464: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10975316052163597275\n",
      "2024-02-02 00:12:38.213470: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 18258293499728254007\n",
      "2024-02-02 00:12:38.213476: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n",
      "2024-02-02 00:12:38.213482: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7571689431104767044\n",
      "2024-02-02 00:12:38.213492: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15932275233087868598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7485 - auc: 0.8559 - loss: 0.3862 - precision: 0.6793 - recall: 0.9421 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m  49/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7449 - auc: 0.8563 - loss: 0.3856 - precision: 0.6883 - recall: 0.9139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:12:57.929509: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7485 - auc: 0.8560 - loss: 0.3862 - precision: 0.6791 - recall: 0.9429 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7489 - auc: 0.8563 - loss: 0.3861 - precision: 0.6794 - recall: 0.9432 - val_accuracy: 0.7538 - val_auc: 0.8595 - val_loss: 0.3814 - val_precision: 0.6709 - val_recall: 0.9977 - learning_rate: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:13:35.919408: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4457327716037455207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 408.24 seconds\n",
      "Model training finished\n",
      "Validation accuracy: 75.38%\n",
      "Inference time: 0.78 seconds\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.51      0.67    104121\n",
      "           1       0.67      1.00      0.80    104360\n",
      "\n",
      "    accuracy                           0.75    208481\n",
      "   macro avg       0.83      0.75      0.74    208481\n",
      "weighted avg       0.83      0.75      0.74    208481\n",
      "\n",
      "AUC ROC Score: 0.7535490051287699\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "## Prepare the data\n",
    "\n",
    "data_columns = list(data.columns)\n",
    "\n",
    "CSV_HEADER = data_columns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = data_columns[1:len(data_columns)-2]\n",
    "\n",
    "\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"protocol\": sorted(list(data_train[\"protocol\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"is_attack\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"yes\", \"no\"]\n",
    "\n",
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 4  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "lookup_dict = {}\n",
    "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "    # Create a lookup to convert a string values to an integer indices.\n",
    "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "    lookup = layers.StringLookup(\n",
    "        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "    )\n",
    "    lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "def encode_categorical(batch_x, batch_y, weights):\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name=TARGET_FEATURE_NAME,\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "        .map(encode_categorical)\n",
    "    )\n",
    "    return dataset.cache()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "    callbacks=None,\n",
    "):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "                ],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset,callbacks=callbacks\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training finished in {training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Model training finished\")\n",
    "    start_time = time.time()\n",
    "    loss, accuracy, precision, recall, auc  = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "    # Assuming your model's final layer is a sigmoid, as you're using BinaryCrossentropy\n",
    "    # If it's softmax, you'll need to adapt this and use `predict_classes` or similar\n",
    "    y_pred = (model.predict(validation_dataset) > 0.5).astype(\"int32\")\n",
    "    y_true = []\n",
    "    for x, y, w in validation_dataset:\n",
    "        y_true.extend(y.numpy())\n",
    "    y_true = np.array(y_true).flatten()  # Ensure y_true is a flat array\n",
    "    assert len(y_true) == len(y_pred), \"Mismatch in the number of samples between y_true and y_pred\"\n",
    "\n",
    "\n",
    "    #print(f\"Validation accuracy: {results[1]:.2f}%\")\n",
    "    #print(f\"AUC: {results[-1]}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Assuming binary classification and y_true and y_pred are numpy arrays\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"AUC ROC Score: {auc_score}\")\n",
    "\n",
    "    return history, training_time, inference_time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create model inputs\n",
    "\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer()),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Experiment 1: a baseline model\n",
    "\n",
    "In the first experiment, we create a simple multi-layer feed-forward network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization,\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization,\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the baseline model:\n",
    "\"\"\"\n",
    "\n",
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## Experiment 2: TabTransformer\n",
    "\n",
    "\n",
    "\n",
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "\n",
    "\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=partial(\n",
    "                layers.LayerNormalization, epsilon=1e-6\n",
    "            ),  # using partial to provide keyword arguments before initialization\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Batch Normalization before the MultiHead Attention\n",
    "        encoded_categorical_features = layers.BatchNormalization()(encoded_categorical_features)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization,\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "# Define callbacks for learning rate scheduling and early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
    "\n",
    "# ...\n",
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping_callback, reduce_lr_callback]  # Add callbacks here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IGvFYQs8Dkq_",
    "outputId": "60a7aded-919e-49e5-ec52-3b5a6c5cc507"
   },
   "outputs": [],
   "source": [
    "# def plot_learning_curves(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "\n",
    "#     epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#     plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#     plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# history, training_time, inference_time = run_experiment(\n",
    "#         model=tabtransformer_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[early_stopping_callback, reduce_lr_callback]\n",
    "# )\n",
    "\n",
    "# plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "m5B3AZ-9DkrB"
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# ## Prepare the data\n",
    "\n",
    "# data_columns = list(data.columns)\n",
    "\n",
    "# CSV_HEADER = data_columns\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Define dataset metadata\n",
    "\n",
    "# Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "# the data into input features, and encoding the input features with respect to their types.\n",
    "# \"\"\"\n",
    "\n",
    "# # A list of the numerical feature names.\n",
    "# NUMERIC_FEATURE_NAMES = data_columns[1:len(data_columns)-2]\n",
    "\n",
    "\n",
    "# # A dictionary of the categorical features and their vocabulary.\n",
    "# CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "#     \"protocol\": sorted(list(data_train[\"protocol\"].unique())),\n",
    "# }\n",
    "# # Name of the column to be used as instances weight.\n",
    "# WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# # A list of the categorical feature names.\n",
    "# CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# # A list of all the input features.\n",
    "# FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# # A list of column default values for each feature.\n",
    "# COLUMN_DEFAULTS = [\n",
    "#     [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "#     for feature_name in CSV_HEADER\n",
    "# ]\n",
    "# # The name of the target feature.\n",
    "# TARGET_FEATURE_NAME = \"is_attack\"\n",
    "# # A list of the labels of the target features.\n",
    "# TARGET_LABELS = [\"yes\", \"no\"]\n",
    "\n",
    "# \"\"\"\n",
    "# ## Configure the hyperparameters\n",
    "\n",
    "# The hyperparameters includes model architecture and training configurations.\n",
    "# \"\"\"\n",
    "\n",
    "# LEARNING_RATE = 0.001\n",
    "# WEIGHT_DECAY = 0.0001\n",
    "# DROPOUT_RATE = 0.2\n",
    "# BATCH_SIZE = 265\n",
    "# NUM_EPOCHS = 20\n",
    "\n",
    "# NUM_TRANSFORMER_BLOCKS = 4  # Number of transformer blocks.\n",
    "# NUM_HEADS = 4  # Number of attention heads.\n",
    "# EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "# MLP_HIDDEN_UNITS_FACTORS = [\n",
    "#     2,\n",
    "#     1,\n",
    "# ]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "# NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement data reading pipeline\n",
    "\n",
    "# We define an input function that reads and parses the file, then converts features\n",
    "# and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "# for training or evaluation.\n",
    "# \"\"\"\n",
    "\n",
    "# target_label_lookup = layers.StringLookup(\n",
    "#     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    "# )\n",
    "\n",
    "\n",
    "# def prepare_example(features, target):\n",
    "#     target_index = target_label_lookup(target)\n",
    "#     weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "#     return features, target_index, weights\n",
    "\n",
    "\n",
    "# lookup_dict = {}\n",
    "# for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#     vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "#     # Create a lookup to convert a string values to an integer indices.\n",
    "#     # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "#     # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "#     lookup = layers.StringLookup(\n",
    "#         vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "#     )\n",
    "#     lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "# def encode_categorical(batch_x, batch_y, weights):\n",
    "#     for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#         batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "#     return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "# def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "#     dataset = (\n",
    "#         tf_data.experimental.make_csv_dataset(\n",
    "#             csv_file_path,\n",
    "#             batch_size=batch_size,\n",
    "#             column_names=CSV_HEADER,\n",
    "#             column_defaults=COLUMN_DEFAULTS,\n",
    "#             label_name=TARGET_FEATURE_NAME,\n",
    "#             num_epochs=1,\n",
    "#             header=False,\n",
    "#             na_value=\"?\",\n",
    "#             shuffle=shuffle,\n",
    "#         )\n",
    "#         .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "#         .map(encode_categorical)\n",
    "#     )\n",
    "#     return dataset.cache()\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement a training and evaluation procedure\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def run_experiment(\n",
    "#     model,\n",
    "#     train_data_file,\n",
    "#     test_data_file,\n",
    "#     num_epochs,\n",
    "#     learning_rate,\n",
    "#     weight_decay,\n",
    "#     batch_size,\n",
    "#     callbacks=None,\n",
    "# ):\n",
    "#     optimizer = keras.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.BinaryCrossentropy(),\n",
    "#         metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "#     )\n",
    "\n",
    "#     train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "#     validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "#     print(\"Start training the model...\")\n",
    "#     history = model.fit(\n",
    "#         train_dataset, epochs=num_epochs, validation_data=validation_dataset,callbacks=callbacks\n",
    "#     )\n",
    "#     print(\"Model training finished\")\n",
    "\n",
    "#     _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "#     print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "#     return history\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Create model inputs\n",
    "\n",
    "# Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "# and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "# and data type.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_model_inputs():\n",
    "#     inputs = {}\n",
    "#     for feature_name in FEATURE_NAMES:\n",
    "#         if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "#             inputs[feature_name] = layers.Input(\n",
    "#                 name=feature_name, shape=(), dtype=\"float32\"\n",
    "#             )\n",
    "#         else:\n",
    "#             inputs[feature_name] = layers.Input(\n",
    "#                 name=feature_name, shape=(), dtype=\"float32\"\n",
    "#             )\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Encode features\n",
    "\n",
    "# The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "# We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "# regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def encode_inputs(inputs, embedding_dims):\n",
    "#     encoded_categorical_feature_list = []\n",
    "#     numerical_feature_list = []\n",
    "\n",
    "#     for feature_name in inputs:\n",
    "#         if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#             vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "#             # Create a lookup to convert a string values to an integer indices.\n",
    "#             # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "#             # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "\n",
    "#             # Convert the string input values into integer indices.\n",
    "\n",
    "#             # Create an embedding layer with the specified dimensions.\n",
    "#             embedding = layers.Embedding(\n",
    "#                 input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "#             )\n",
    "\n",
    "#             # Convert the index values to embedding representations.\n",
    "#             encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "#             encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "#         else:\n",
    "#             # Use the numerical features as-is.\n",
    "#             numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "#             numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "#     return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement an MLP block\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "#     mlp_layers = []\n",
    "#     for units in hidden_units:\n",
    "#         mlp_layers.append(normalization_layer()),\n",
    "#         mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "#         mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "#     return keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Experiment 1: a baseline model\n",
    "\n",
    "# In the first experiment, we create a simple multi-layer feed-forward network.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_baseline_model(\n",
    "#     embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "# ):\n",
    "#     # Create model inputs.\n",
    "#     inputs = create_model_inputs()\n",
    "#     # encode features.\n",
    "#     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "#         inputs, embedding_dims\n",
    "#     )\n",
    "#     # Concatenate all features.\n",
    "#     features = layers.concatenate(\n",
    "#         encoded_categorical_feature_list + numerical_feature_list\n",
    "#     )\n",
    "#     # Compute Feedforward layer units.\n",
    "#     feedforward_units = [features.shape[-1]]\n",
    "\n",
    "#     # Create several feedforwad layers with skip connections.\n",
    "#     for layer_idx in range(num_mlp_blocks):\n",
    "#         features = create_mlp(\n",
    "#             hidden_units=feedforward_units,\n",
    "#             dropout_rate=dropout_rate,\n",
    "#             activation=keras.activations.gelu,\n",
    "#             normalization_layer=layers.LayerNormalization,\n",
    "#             name=f\"feedforward_{layer_idx}\",\n",
    "#         )(features)\n",
    "\n",
    "#     # Compute MLP hidden_units.\n",
    "#     mlp_hidden_units = [\n",
    "#         factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "#     ]\n",
    "#     # Create final MLP.\n",
    "#     features = create_mlp(\n",
    "#         hidden_units=mlp_hidden_units,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         activation=keras.activations.selu,\n",
    "#         normalization_layer=layers.BatchNormalization,\n",
    "#         name=\"MLP\",\n",
    "#     )(features)\n",
    "\n",
    "#     # Add a sigmoid as a binary classifer.\n",
    "#     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# baseline_model = create_baseline_model(\n",
    "#     embedding_dims=EMBEDDING_DIMS,\n",
    "#     num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "#     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# print(\"Total model weights:\", baseline_model.count_params())\n",
    "# keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "# \"\"\"\n",
    "# Let's train and evaluate the baseline model:\n",
    "# \"\"\"\n",
    "\n",
    "# history = run_experiment(\n",
    "#     model=baseline_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# ## Experiment 2: TabTransformer\n",
    "\n",
    "\n",
    "\n",
    "# def create_tabtransformer_classifier(\n",
    "#     num_transformer_blocks,\n",
    "#     num_heads,\n",
    "#     embedding_dims,\n",
    "#     mlp_hidden_units_factors,\n",
    "#     dropout_rate,\n",
    "#     use_column_embedding=False,\n",
    "# ):\n",
    "#     # Create model inputs.\n",
    "#     inputs = create_model_inputs()\n",
    "#     # encode features.\n",
    "#     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "#         inputs, embedding_dims\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Stack categorical feature embeddings for the Tansformer.\n",
    "#     encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "#     # Concatenate numerical features.\n",
    "#     numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "#     # Add column embedding to categorical feature embeddings.\n",
    "#     if use_column_embedding:\n",
    "#         num_columns = encoded_categorical_features.shape[1]\n",
    "#         column_embedding = layers.Embedding(\n",
    "#             input_dim=num_columns, output_dim=embedding_dims\n",
    "#         )\n",
    "#         column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "#         encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "#             column_indices\n",
    "#         )\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for block_idx in range(num_transformer_blocks):\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads,\n",
    "#             key_dim=embedding_dims,\n",
    "#             dropout=dropout_rate,\n",
    "#             name=f\"multihead_attention_{block_idx}\",\n",
    "#         )(encoded_categorical_features, encoded_categorical_features)\n",
    "#         # Skip connection 1.\n",
    "#         x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "#             [attention_output, encoded_categorical_features]\n",
    "#         )\n",
    "#         # Layer normalization 1.\n",
    "#         x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "#         # Feedforward.\n",
    "#         feedforward_output = create_mlp(\n",
    "#             hidden_units=[embedding_dims],\n",
    "#             dropout_rate=dropout_rate,\n",
    "#             activation=keras.activations.gelu,\n",
    "#             normalization_layer=partial(\n",
    "#                 layers.LayerNormalization, epsilon=1e-6\n",
    "#             ),  # using partial to provide keyword arguments before initialization\n",
    "#             name=f\"feedforward_{block_idx}\",\n",
    "#         )(x)\n",
    "#         # Skip connection 2.\n",
    "#         x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "#         # Layer normalization 2.\n",
    "#         encoded_categorical_features = layers.LayerNormalization(\n",
    "#             name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "#         )(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "#     categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "#     # Apply layer normalization to the numerical features.\n",
    "#     numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "#     # Prepare the input for the final MLP block.\n",
    "#     features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for block_idx in range(num_transformer_blocks):\n",
    "#         # Batch Normalization before the MultiHead Attention\n",
    "#         encoded_categorical_features = layers.BatchNormalization()(encoded_categorical_features)\n",
    "\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads,\n",
    "#             key_dim=embedding_dims,\n",
    "#             dropout=dropout_rate,\n",
    "#             name=f\"multihead_attention_{block_idx}\",\n",
    "#         )(encoded_categorical_features, encoded_categorical_features)\n",
    "\n",
    "#         # Skip connection 1.\n",
    "#         x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "#             [attention_output, encoded_categorical_features]\n",
    "#         )\n",
    "#         # Layer normalization 1.\n",
    "#         x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Compute MLP hidden_units.\n",
    "#     mlp_hidden_units = [\n",
    "#         factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "#     ]\n",
    "#     # Create final MLP.\n",
    "#     features = create_mlp(\n",
    "#         hidden_units=mlp_hidden_units,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         activation=keras.activations.selu,\n",
    "#         normalization_layer=layers.BatchNormalization,\n",
    "#         name=\"MLP\",\n",
    "#     )(features)\n",
    "\n",
    "#     # Add a sigmoid as a binary classifer.\n",
    "#     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tabtransformer_model = create_tabtransformer_classifier(\n",
    "#     num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     embedding_dims=EMBEDDING_DIMS,\n",
    "#     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "# keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "# \"\"\"\n",
    "# Let's train and evaluate the TabTransformer model:\n",
    "# \"\"\"\n",
    "# # Define callbacks for learning rate scheduling and early stopping\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
    "\n",
    "# # ...\n",
    "# history = run_experiment(\n",
    "#     model=tabtransformer_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[early_stopping_callback, reduce_lr_callback]  # Add callbacks here\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7aKpTARDkrD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQvxY0FlDkrD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3157581,
     "sourceId": 5466512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4137697,
     "sourceId": 7163258,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

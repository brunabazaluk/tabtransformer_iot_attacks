{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5466512,
          "sourceType": "datasetVersion",
          "datasetId": 3157581
        },
        {
          "sourceId": 7163258,
          "sourceType": "datasetVersion",
          "datasetId": 4137697
        }
      ],
      "dockerImageVersionId": 30615,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "root_dir = \"/content/gdrive/MyDrive/\"\n",
        "\n"
      ],
      "metadata": {
        "id": "oj1L3ykp4cqI",
        "execution": {
          "iopub.status.busy": "2023-12-09T18:16:00.334576Z",
          "iopub.execute_input": "2023-12-09T18:16:00.335039Z",
          "iopub.status.idle": "2023-12-09T18:16:00.809061Z",
          "shell.execute_reply.started": "2023-12-09T18:16:00.334995Z",
          "shell.execute_reply": "2023-12-09T18:16:00.806268Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "NzkN2SJNUCUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "ObZlaQWd5FuQ",
        "execution": {
          "iopub.status.busy": "2023-12-11T01:07:44.746232Z",
          "iopub.execute_input": "2023-12-11T01:07:44.747175Z",
          "iopub.status.idle": "2023-12-11T01:07:46.959774Z",
          "shell.execute_reply.started": "2023-12-11T01:07:44.747131Z",
          "shell.execute_reply": "2023-12-11T01:07:46.958709Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip gdrive/MyDrive/archive.zip"
      ],
      "metadata": {
        "id": "8mMzBPqSOobK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 250\n",
        "bcf_ddos = pd.read_csv('gdrive/MyDrive/BF1_DDoS_AD_1.csv')\n",
        "bcf_dos = pd.read_csv('gdrive/MyDrive/BF1_DoS_AD_12.csv')\n",
        "\n",
        "will_ddos = pd.read_csv('gdrive/MyDrive/WILL_DDoS_AD_1.csv')\n",
        "will_dos = pd.read_csv('gdrive/MyDrive/WILL_DoS_AD_11.csv')\n",
        "\n",
        "dcf_ddos = pd.read_csv('gdrive/MyDrive/Delay_DDoS_AD_1.csv')\n",
        "dcf_dos = pd.read_csv('gdrive/MyDrive/Delay_DoS_AD_11.csv')\n",
        "\n",
        "isf_ddos = pd.read_csv('gdrive/MyDrive/Sub_DDoS_AD_1.csv')\n",
        "isf_dos = pd.read_csv('gdrive/MyDrive/Sub_DoS_AD_11.csv')\n",
        "\n",
        "syn_ddos = pd.read_csv('gdrive/MyDrive/SYN_DDoS-AD_1.csv')\n",
        "syn_dos = pd.read_csv('gdrive/MyDrive/SYN_DoS_AD_1.csv')\n",
        "\n",
        "l_bcf_ddos = len(bcf_ddos)\n",
        "l_bcf_dos = len(bcf_dos)\n",
        "\n",
        "l_will_ddos = len(will_ddos)\n",
        "l_will_dos = len(will_dos)\n",
        "\n",
        "l_dcf_ddos = len(dcf_ddos)\n",
        "l_dcf_dos = len(dcf_dos)\n",
        "\n",
        "l_isf_ddos = len(isf_ddos)\n",
        "l_isf_dos = len(isf_dos)\n",
        "\n",
        "l_syn_ddos = len(syn_ddos)\n",
        "l_syn_dos = len(syn_dos)\n",
        "\n",
        "classif_bcf_ddos = ['bcf_ddos' for i in range(l_bcf_ddos)]\n",
        "classif_bcf_dos = ['bcf_dos' for i in range(l_bcf_dos)]\n",
        "\n",
        "classif_will_ddos = ['will_ddos' for i in range(l_will_ddos)]\n",
        "classif_will_dos =  ['will_dos' for i in range(l_will_dos)]\n",
        "\n",
        "classif_dcf_ddos = ['dcf_ddos' for i in range(l_dcf_ddos)]\n",
        "classif_dcf_dos =  ['dcf_dos' for i in range(l_dcf_dos)]\n",
        "\n",
        "classif_isf_ddos = ['isf_ddos' for i in range(l_isf_ddos)]\n",
        "classif_isf_dos =  ['isf_dos' for i in range(l_isf_dos)]\n",
        "\n",
        "classif_syn_ddos = ['syn_ddos' for i in range(l_syn_ddos)]\n",
        "classif_syn_dos =  ['syn_dos' for i in range(l_syn_dos)]\n",
        "\n",
        "\n",
        "bcf_ddos['Classification'] = classif_bcf_ddos\n",
        "bcf_dos['Classification'] = classif_bcf_dos\n",
        "\n",
        "will_ddos['Classification'] = classif_will_ddos\n",
        "will_dos['Classification'] = classif_will_dos\n",
        "\n",
        "dcf_ddos['Classification'] = classif_dcf_ddos\n",
        "dcf_dos['Classification'] = classif_dcf_dos\n",
        "\n",
        "isf_ddos['Classification'] = classif_isf_ddos\n",
        "isf_dos['Classification'] = classif_isf_dos\n",
        "\n",
        "syn_ddos['Classification'] = classif_syn_ddos\n",
        "syn_dos['Classification'] = classif_syn_dos\n",
        "\n",
        "\n",
        "data = pd.concat([bcf_ddos,\n",
        "                  #bcf_dos,\n",
        "                  will_ddos,\n",
        "                  #will_dos,\n",
        "                  dcf_ddos,\n",
        "                  #dcf_dos,\n",
        "                  isf_ddos,\n",
        "                  #isf_dos,\n",
        "                  syn_ddos,\n",
        "                  #syn_dos\n",
        "                  ], axis=0)\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "data\n"
      ],
      "metadata": {
        "id": "m7fo8RA1BGXu",
        "execution": {
          "iopub.status.busy": "2023-12-11T01:07:46.961995Z",
          "iopub.execute_input": "2023-12-11T01:07:46.962882Z",
          "iopub.status.idle": "2023-12-11T01:07:57.124500Z",
          "shell.execute_reply.started": "2023-12-11T01:07:46.962848Z",
          "shell.execute_reply": "2023-12-11T01:07:57.123517Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=[\"No.\", \"Message Type\", \"QoS Level\", \"QoS Level.1\", \"Requested QoS\", \"Clean Session Flag\",\n",
        "                          \"Keep Alive\", \"User Name Length\", \"Password Length\", \"Retain\", \"Clean Session Flag.1\",\n",
        "                          \"Will Retain\", \"Will Flag\", \"Will Message Length\", \"Will Topic Length\", \"Topic Length\",\n",
        "                          \"Msg Len\"])\n",
        "#data = data.drop(columns=[\"Epoch Time\",\"Time delta from previous displayed frame\",\n",
        "#                          \"Time since reference or first frame\",\"Frame length on the wire.1\",\"Stream index\",\n",
        "#                          \"iRTT\",\"Time since first frame in this TCP stream\",\"TCP Segment Len\",\n",
        "#                          \"Calculated window size\",\"Syn\",\"Reset\",\"Acknowledgment\",\"Time\",\"Destination\"])\n",
        "data = data.dropna().sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#classif = data['Classification']\n",
        "#data = data = data.drop(columns=['Classification'])\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "XWIU6RCfHFvf",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:08:54.718439Z",
          "iopub.execute_input": "2023-12-09T19:08:54.719040Z",
          "iopub.status.idle": "2023-12-09T19:08:54.862742Z",
          "shell.execute_reply.started": "2023-12-09T19:08:54.718987Z",
          "shell.execute_reply": "2023-12-09T19:08:54.860948Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = data.loc[:int(0.8*len(data))]\n",
        "X_test = data.loc[int(0.8*len(data)):(int(0.8*len(data))+int(0.1*len(data)))]\n",
        "X_valid = data.loc[(int(0.8*len(data))+int(0.1*len(data))):]\n",
        "\n",
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "valid_data_file = \"test_data.csv\"\n",
        "\n",
        "X_train.to_csv(train_data_file, index=False, header=False)\n",
        "X_test.to_csv(test_data_file, index=False, header=False)\n",
        "X_valid.to_csv(valid_data_file, index=False, header=False)\n"
      ],
      "metadata": {
        "id": "uXFO20Y14sQa",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:09:31.245741Z",
          "iopub.execute_input": "2023-12-09T19:09:31.246406Z",
          "iopub.status.idle": "2023-12-09T19:09:48.074348Z",
          "shell.execute_reply.started": "2023-12-09T19:09:31.246372Z",
          "shell.execute_reply": "2023-12-09T19:09:48.072802Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER = [\"Epoch Time\",\n",
        "              \"Protocol\",\n",
        "              \"Source\",\n",
        "              \"Frame length on the wire\",\n",
        "              \"Time delta from previous displayed frame\",\n",
        "              \"Time since reference or first frame\",\n",
        "              \"Frame length on the wire.1\",\n",
        "              \"Stream index\",\n",
        "              \"iRTT\",\n",
        "              \"Time since first frame in this TCP stream\",\n",
        "              \"TCP Segment Len\",\n",
        "              \"Calculated window size\",\n",
        "              \"Syn\",\n",
        "              \"Reset\",\n",
        "              \"Acknowledgment\",\n",
        "              \"Info\",\n",
        "              \"Classification\"\n",
        "              ]\n",
        "\n",
        "# A list of the numerical feature names.\n",
        "NUMERIC_FEATURE_NAMES = [\n",
        "    \"Epoch Time\",\n",
        "    \"Frame length on the wire\",\n",
        "    \"Time delta from previous displayed frame\",\n",
        "    \"Time since reference or first frame\",\n",
        "    \"Frame length on the wire.1\",\n",
        "    \"Stream index\",\n",
        "    \"iRTT\",\n",
        "    \"Time since first frame in this TCP stream\",\n",
        "    \"TCP Segment Len\",\n",
        "    \"Calculated window size\",\n",
        "]\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"Protocol\": sorted(list(X_train[\"Protocol\"].unique())),\n",
        "    \"Source\": sorted(list(X_train[\"Source\"].unique())),\n",
        "    \"Syn\": sorted(list(X_train[\"Syn\"].unique())),\n",
        "    \"Reset\": sorted(list(X_train[\"Reset\"].unique())),\n",
        "    \"Acknowledgment\": sorted(list(X_train[\"Acknowledgment\"].unique())),\n",
        "    \"Info\": sorted(list(X_train[\"Info\"].unique())),\n",
        "}\n",
        "# Name of the column to be used as instances weight.\n",
        "#WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    #[0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"Classification\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = ['bcf_ddos',\n",
        "                #'bcf_dos',\n",
        "                'will_ddos',\n",
        "                #'will_dos',\n",
        "                'dcf_ddos',\n",
        "                #'dcf_dos',\n",
        "                'isf_ddos',\n",
        "                #'isf_dos',\n",
        "                'syn_ddos',\n",
        "                #'syn_dos'\n",
        "                 ]\n"
      ],
      "metadata": {
        "id": "voftg69d5lh5",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:09:48.075992Z",
          "iopub.execute_input": "2023-12-09T19:09:48.076314Z",
          "iopub.status.idle": "2023-12-09T19:09:51.409503Z",
          "shell.execute_reply.started": "2023-12-09T19:09:48.076286Z",
          "shell.execute_reply": "2023-12-09T19:09:51.408154Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.2\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "NUM_TRANSFORMER_BLOCKS = 10  # Number of transformer blocks.\n",
        "NUM_HEADS = 4  # Number of attention heads.\n",
        "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
        "MLP_HIDDEN_UNITS_FACTORS = [\n",
        "    2,\n",
        "    1,\n",
        "]  # MLP hidden layer units, as factors of the number of inputs.\n",
        "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n"
      ],
      "metadata": {
        "id": "jBvFNqW3JuEH",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:09:51.410921Z",
          "iopub.execute_input": "2023-12-09T19:09:51.411238Z",
          "iopub.status.idle": "2023-12-09T19:09:51.418524Z",
          "shell.execute_reply.started": "2023-12-09T19:09:51.411209Z",
          "shell.execute_reply": "2023-12-09T19:09:51.417134Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_label_lookup = layers.StringLookup(\n",
        "    vocabulary=TARGET_LABELS, num_oov_indices=0,\n",
        "    #mask_token=None,\n",
        "    mask_token=None,\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_example(features, target):\n",
        "    target_index = target_label_lookup(target)\n",
        "    #weights = features.pop(WEIGHT_COLUMN_NAME)\n",
        "    return features, target_index#, weights\n",
        "\n",
        "lookup_dict = {}\n",
        "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "    # Create a lookup to convert a string values to an integer indices.\n",
        "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "    lookup = layers.StringLookup(\n",
        "        vocabulary=vocabulary, num_oov_indices=0,\n",
        "        mask_token=None\n",
        "    )\n",
        "    lookup_dict[feature_name] = lookup\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "    return dataset.cache()\n"
      ],
      "metadata": {
        "id": "lzC5XetqJ0kj",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:09:51.421913Z",
          "iopub.execute_input": "2023-12-09T19:09:51.422282Z",
          "iopub.status.idle": "2023-12-09T19:10:46.426266Z",
          "shell.execute_reply.started": "2023-12-09T19:09:51.422252Z",
          "shell.execute_reply": "2023-12-09T19:10:46.423972Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(\n",
        "    model,\n",
        "    train_data_file,\n",
        "    test_data_file,\n",
        "    num_epochs,\n",
        "    learning_rate,\n",
        "    weight_decay,\n",
        "    batch_size,\n",
        "):\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"), keras.metrics.Precision(),\n",
        "                 keras.metrics.Recall(), ],\n",
        "    )\n",
        "\n",
        "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
        "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(\n",
        "        #print(train_dataset)\n",
        "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
        "    )\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
        "\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "qA23v1qoJ6ay",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:10:46.428649Z",
          "iopub.execute_input": "2023-12-09T19:10:46.430192Z",
          "iopub.status.idle": "2023-12-09T19:10:46.441788Z",
          "shell.execute_reply.started": "2023-12-09T19:10:46.430139Z",
          "shell.execute_reply": "2023-12-09T19:10:46.440206Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "D5t69aqaJ_Er",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:10:46.444229Z",
          "iopub.execute_input": "2023-12-09T19:10:46.445100Z",
          "iopub.status.idle": "2023-12-09T19:10:46.464458Z",
          "shell.execute_reply.started": "2023-12-09T19:10:46.445043Z",
          "shell.execute_reply": "2023-12-09T19:10:46.462366Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_inputs(inputs, embedding_dims):\n",
        "\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "\n",
        "            # Get the vocabulary of the categorical feature.\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            if feature_name == \"Info\":\n",
        "                df = pd.DataFrame(vocabulary)\n",
        "                df.to_csv(\"vocab.csv\")\n",
        "\n",
        "            # Create a lookup to convert string values to an integer indices.\n",
        "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
        "            lookup = layers.StringLookup(\n",
        "                vocabulary=vocabulary,\n",
        "                mask_token=None,\n",
        "                num_oov_indices=0,\n",
        "                output_mode=\"int\",\n",
        "            )\n",
        "\n",
        "            # Convert the string input values into integer indices.\n",
        "            encoded_feature = lookup(inputs[feature_name])\n",
        "\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
        "            )\n",
        "\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_categorical_feature = embedding(encoded_feature)\n",
        "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Use the numerical features as-is.\n",
        "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
        "            numerical_feature_list.append(numerical_feature)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list\n"
      ],
      "metadata": {
        "id": "l5MiyWPyKB9H",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:10:46.467568Z",
          "iopub.execute_input": "2023-12-09T19:10:46.469194Z",
          "iopub.status.idle": "2023-12-09T19:10:46.482664Z",
          "shell.execute_reply.started": "2023-12-09T19:10:46.469115Z",
          "shell.execute_reply": "2023-12-09T19:10:46.481436Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
        "\n",
        "    mlp_layers = []\n",
        "    for units in hidden_units:\n",
        "        mlp_layers.append(normalization_layer),\n",
        "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
        "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    return keras.Sequential(mlp_layers, name=name)\n"
      ],
      "metadata": {
        "id": "jw-yZB3DKIXk",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:10:46.483965Z",
          "iopub.execute_input": "2023-12-09T19:10:46.484360Z",
          "iopub.status.idle": "2023-12-09T19:10:46.502721Z",
          "shell.execute_reply.started": "2023-12-09T19:10:46.484329Z",
          "shell.execute_reply": "2023-12-09T19:10:46.500954Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tabtransformer_classifier(\n",
        "    num_transformer_blocks,\n",
        "    num_heads,\n",
        "    embedding_dims,\n",
        "    mlp_hidden_units_factors,\n",
        "    dropout_rate,\n",
        "    use_column_embedding=False,\n",
        "):\n",
        "\n",
        "    # Create model inputs.\n",
        "    inputs = create_model_inputs()\n",
        "    # encode features.\n",
        "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
        "        inputs, embedding_dims\n",
        "    )\n",
        "    # Stack categorical feature embeddings for the Tansformer.\n",
        "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
        "    # Concatenate numerical features.\n",
        "    numerical_features = layers.concatenate(numerical_feature_list)\n",
        "\n",
        "    # Add column embedding to categorical feature embeddings.\n",
        "    if use_column_embedding:\n",
        "        num_columns = encoded_categorical_features.shape[1]\n",
        "        column_embedding = layers.Embedding(\n",
        "            input_dim=num_columns, output_dim=embedding_dims\n",
        "        )\n",
        "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
        "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
        "            column_indices\n",
        "        )\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for block_idx in range(num_transformer_blocks):\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dims,\n",
        "            dropout=dropout_rate,\n",
        "            name=f\"multihead_attention_{block_idx}\",\n",
        "        )(encoded_categorical_features, encoded_categorical_features)\n",
        "        # Skip connection 1.\n",
        "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
        "            [attention_output, encoded_categorical_features]\n",
        "        )\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
        "        # Feedforward.\n",
        "        feedforward_output = create_mlp(\n",
        "            hidden_units=[embedding_dims],\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation=keras.activations.gelu,\n",
        "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
        "            name=f\"feedforward_{block_idx}\",\n",
        "        )(x)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
        "        # Layer normalization 2.\n",
        "        encoded_categorical_features = layers.LayerNormalization(\n",
        "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
        "        )(x)\n",
        "\n",
        "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
        "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
        "    # Apply layer normalization to the numerical features.\n",
        "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
        "    # Prepare the input for the final MLP block.\n",
        "    features = layers.concatenate([categorical_features, numerical_features])\n",
        "\n",
        "    # Compute MLP hidden_units.\n",
        "    mlp_hidden_units = [\n",
        "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
        "    ]\n",
        "    # Create final MLP.\n",
        "    features = create_mlp(\n",
        "        hidden_units=mlp_hidden_units,\n",
        "        dropout_rate=dropout_rate,\n",
        "        activation=keras.activations.selu,\n",
        "        normalization_layer=layers.BatchNormalization(),\n",
        "        name=\"MLP\",\n",
        "    )(features)\n",
        "\n",
        "    # Add a sigmoid as a binary classifer.\n",
        "    outputs = layers.Dense(units=1, activation=\"softmax\", name=\"softmax\")(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "tabtransformer_model = create_tabtransformer_classifier(\n",
        "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embedding_dims=EMBEDDING_DIMS,\n",
        "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        ")\n",
        "\n",
        "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
        "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n"
      ],
      "metadata": {
        "id": "Jac3r5qdKK04",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:10:46.504870Z",
          "iopub.execute_input": "2023-12-09T19:10:46.505278Z",
          "iopub.status.idle": "2023-12-09T19:11:54.615690Z",
          "shell.execute_reply.started": "2023-12-09T19:10:46.505245Z",
          "shell.execute_reply": "2023-12-09T19:11:54.614129Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = run_experiment(\n",
        "    model=tabtransformer_model,\n",
        "    train_data_file=train_data_file,\n",
        "    test_data_file=test_data_file,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "QgyCnVZ4KQI2",
        "execution": {
          "iopub.status.busy": "2023-12-09T19:11:54.617914Z",
          "iopub.execute_input": "2023-12-09T19:11:54.618357Z",
          "iopub.status.idle": "2023-12-09T19:13:21.044680Z",
          "shell.execute_reply.started": "2023-12-09T19:11:54.618322Z",
          "shell.execute_reply": "2023-12-09T19:13:21.041732Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "6lrMTxY1nCvD"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow --upgrade\n",
    "#!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "_Nm8d-39nDCr",
    "outputId": "ebfbd14a-3319-4539-e039-141eaf159614"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import data as tf_data\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "uk5kQtrVnDFN"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# root_dir = \"/content/gdrive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qeZMWoyRnDHm"
   },
   "outputs": [],
   "source": [
    "# !unzip gdrive/MyDrive/packet_features.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWxegZ-AnDKK",
    "outputId": "73ac51c7-80a6-48db-b5ce-1fe803093ed6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12469/509904704.py:3: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mqtt_bruteforce = pd.read_csv('packet_features/mqtt_bruteforce.csv')\n",
      "/tmp/ipykernel_12469/509904704.py:4: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sparta = pd.read_csv('packet_features/sparta.csv')\n",
      "/tmp/ipykernel_12469/509904704.py:5: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv('packet_features/normal.csv')\n"
     ]
    }
   ],
   "source": [
    "scana = pd.read_csv('packet_features/scan_A.csv')\n",
    "scansu = pd.read_csv('packet_features/scan_sU.csv')\n",
    "mqtt_bruteforce = pd.read_csv('packet_features/mqtt_bruteforce.csv')\n",
    "sparta = pd.read_csv('packet_features/sparta.csv')\n",
    "normal = pd.read_csv('packet_features/normal.csv')\n",
    "\n",
    "#data = pd.concat([scana, scansu,mqtt_bruteforce,sparta,normal],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "q0Pkvbb4Dkqm",
    "outputId": "4d8b53ff-2e42-49d7-8258-1121ae43a300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "0    70768\n",
       "1    40624\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scana['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ae0eA0jCDkqp",
    "outputId": "4d7f55c8-1f1d-4eeb-d1d4-77f329635e81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "0    210819\n",
       "1     22436\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scansu['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "aquLeUpBDkqr",
    "outputId": "d3961296-3fed-402c-8815-001e3ed7f85f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "1            19103574\n",
       "0              917206\n",
       "1              625369\n",
       "0               29971\n",
       "is_attack          20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparta['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "f0Wn1IwJDkqv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "0            1023463\n",
       "0              32767\n",
       "is_attack          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "oT75qfm_Dkqw",
    "outputId": "2becaa4b-22d8-4f7e-8dbf-04817da43364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "1            9686086\n",
       "1             327056\n",
       "0              31550\n",
       "0                614\n",
       "is_attack         10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqtt_bruteforce['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "9gVCDGquD60A"
   },
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "\n",
    "    data = data.drop(columns=[\"timestamp\",\"src_ip\",\"dst_ip\", \"src_port\",\"dst_port\"])\n",
    "    data['protocol'] = data['protocol'].astype(str)\n",
    "    data['is_attack'] = data['is_attack'].astype(str)\n",
    "\n",
    "    non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in data.columns:\n",
    "        if col not in ['protocol', 'is_attack']:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "    for column in data:\n",
    "        if column not in ['protocol', 'is_attack']:\n",
    "            clean_col = data[column].dropna()\n",
    "            mean = clean_col.mean()\n",
    "            data[column].fillna(mean, inplace=True)\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    data['is_attack'].replace('0', 'no', inplace=True)\n",
    "    data['is_attack'].replace('1', 'yes', inplace=True)\n",
    "\n",
    "    data.loc[data['is_attack'] == 'is_attack', 'is_attack'] = 'yes'\n",
    "\n",
    "    return data\n",
    "\n",
    "def balance_and_concat(d1,d2,d3,d4,normal):\n",
    "    normal_final = pd.DataFrame()\n",
    "    data = pd.DataFrame()\n",
    "    n = normal.copy()\n",
    "    n = n[n.is_attack == 'no']\n",
    "    \n",
    "    #i had problems trying to do a for loop so i had to do each dataset separately\n",
    "\n",
    "    ############################## d1 ################################\n",
    "    d = d1.copy().sort_values(by=['is_attack']).reset_index(drop=True)\n",
    "    d_yes = (d['is_attack'].values == 'yes').sum()\n",
    "    d_no = (d['is_attack'].values == 'no').sum()\n",
    "    print(\"d1\")\n",
    "    print(\"yes: \", d_yes)\n",
    "    print(\"no: \", d_no)\n",
    "    if min(d_yes,d_no) == d_no: #there are remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = d[:d_no*2]\n",
    "        d_remaining_yes = d[d_no*2:]\n",
    "\n",
    "        if len(n) >= len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n[:len(d_remaining_yes)], d_remaining_yes],axis=0).reset_index(drop=True)\n",
    "            n = n[len(d_remaining_yes):].reset_index(drop=True)\n",
    "        elif len(n) > 0 and len(n) < len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n, d_remaining_yes[:len(n)]],axis=0).reset_index(drop=True)\n",
    "            n = pd.DataFrame()\n",
    "    else: #there aren't any remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = pd.concat([d[:d_yes],d[-d_yes:]],axis=0)\n",
    "    data = pd.concat([data,d_final],axis=0)\n",
    "    \n",
    "    print(\"d1_final\")\n",
    "    print(\"yes: \", (d_final['is_attack'].values == 'yes').sum())\n",
    "    print(\"no: \", (d_final['is_attack'].values == 'no').sum())\n",
    "    ############################## d2 ################################\n",
    "    d = d2.copy().sort_values(by=['is_attack']).reset_index(drop=True)\n",
    "    d_yes = (d['is_attack'].values == 'yes').sum()\n",
    "    d_no = (d['is_attack'].values == 'no').sum()\n",
    "    print(\"d2\")\n",
    "    print(\"yes: \", d_yes)\n",
    "    print(\"no: \", d_no)\n",
    "    if min(d_yes,d_no) == d_no: #there are remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = d[:d_no*2]\n",
    "        d_remaining_yes = d[d_no*2:]\n",
    "\n",
    "        if len(n) >= len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n[:len(d_remaining_yes)], d_remaining_yes],axis=0).reset_index(drop=True)\n",
    "            n = n[len(d_remaining_yes):].reset_index(drop=True)\n",
    "        elif len(n) > 0 and len(n) < len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n, d_remaining_yes[:len(n)]],axis=0).reset_index(drop=True)\n",
    "            n = pd.DataFrame()\n",
    "    else: #there aren't any remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = pd.concat([d[:d_yes],d[-d_yes:]],axis=0)\n",
    "    data = pd.concat([data,d_final],axis=0)\n",
    "    \n",
    "    print(\"d2_final\")\n",
    "    print(\"yes: \", (d_final['is_attack'].values == 'yes').sum())\n",
    "    print(\"no: \", (d_final['is_attack'].values == 'no').sum())\n",
    "    ############################## d3 ################################\n",
    "    d = d3.copy().sort_values(by=['is_attack']).reset_index(drop=True)\n",
    "    d_yes = (d['is_attack'].values == 'yes').sum()\n",
    "    d_no = (d['is_attack'].values == 'no').sum()\n",
    "    print(\"d3\")\n",
    "    print(\"yes: \", d_yes)\n",
    "    print(\"no: \", d_no)\n",
    "    if min(d_yes,d_no) == d_no: #there are remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = d[:d_no*2]\n",
    "        d_remaining_yes = d[d_no*2:]\n",
    "\n",
    "        if len(n) >= len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n[:len(d_remaining_yes)], d_remaining_yes],axis=0).reset_index(drop=True)\n",
    "            n = n[len(d_remaining_yes):].reset_index(drop=True)\n",
    "        elif len(n) > 0 and len(n) < len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n, d_remaining_yes[:len(n)]],axis=0).reset_index(drop=True)\n",
    "            n = pd.DataFrame()\n",
    "    else: #there aren't any remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = pd.concat([d[:d_yes],d[-d_yes:]],axis=0)\n",
    "    data = pd.concat([data,d_final],axis=0)\n",
    "    \n",
    "    print(\"d3_final\")\n",
    "    print(\"yes: \", (d_final['is_attack'].values == 'yes').sum())\n",
    "    print(\"no: \", (d_final['is_attack'].values == 'no').sum())\n",
    "    ############################## d4 ################################\n",
    "    d = d4.copy().sort_values(by=['is_attack']).reset_index(drop=True)\n",
    "    d_yes = (d['is_attack'].values == 'yes').sum()\n",
    "    d_no = (d['is_attack'].values == 'no').sum()\n",
    "    print(\"d4\")\n",
    "    print(\"yes: \", d_yes)\n",
    "    print(\"no: \", d_no)\n",
    "    if min(d_yes,d_no) == d_no: #there are remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = d[:d_no*2]\n",
    "        d_remaining_yes = d[d_no*2:]\n",
    "\n",
    "        if len(n) >= len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n[:len(d_remaining_yes)], d_remaining_yes],axis=0).reset_index(drop=True)\n",
    "            n = n[len(d_remaining_yes):].reset_index(drop=True)\n",
    "        elif len(n) > 0 and len(n) < len(d_remaining_yes):\n",
    "            normal_final = pd.concat([normal_final, n, d_remaining_yes[:len(n)]],axis=0).reset_index(drop=True)\n",
    "            n = pd.DataFrame()\n",
    "    else: #there aren't any remaining 'yes' rows to be concatenated with \"normal\"\n",
    "        d_final = pd.concat([d[:d_yes],d[-d_yes:]],axis=0)\n",
    "    data = pd.concat([data,d_final],axis=0)\n",
    "    \n",
    "    print(\"d4_final\")\n",
    "    print(\"yes: \", (d_final['is_attack'].values == 'yes').sum())\n",
    "    print(\"no: \", (d_final['is_attack'].values == 'no').sum())\n",
    "    \n",
    "    data = pd.concat([data,n],axis=0)\n",
    "    n_yes = (normal_final['is_attack'].values == 'yes').sum()\n",
    "    n_no = (normal_final['is_attack'].values == 'no').sum()\n",
    "    print(\"n\")\n",
    "    print(\"yes: \", n_yes)\n",
    "    print(\"no: \", n_no)\n",
    "\n",
    "    #data = pd.concat([d[1], d[2],d[3],d[4],n],axis=0)\n",
    "    \n",
    "    #BALANCE DATASET\n",
    "    #aux = data.groupby('is_attack')\n",
    "    #data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
    "\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    #print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
    "    #print('quantity of yes: ', data['is_attack'].value_counts()[1])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "X0jGHWF5ExO9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12469/1843275389.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(mean, inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('0', 'no', inplace=True)\n",
      "/tmp/ipykernel_12469/1843275389.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['is_attack'].replace('1', 'yes', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1\n",
      "yes:  40624\n",
      "no:  70768\n",
      "d1_final\n",
      "yes:  40624\n",
      "no:  40624\n",
      "d2\n",
      "yes:  22436\n",
      "no:  210819\n",
      "d2_final\n",
      "yes:  22436\n",
      "no:  22436\n",
      "d3\n",
      "yes:  10013152\n",
      "no:  32164\n",
      "d3_final\n",
      "yes:  32164\n",
      "no:  32164\n",
      "d4\n",
      "yes:  19728963\n",
      "no:  947177\n",
      "d4_final\n",
      "yes:  947177\n",
      "no:  947177\n",
      "n\n",
      "yes:  1056230\n",
      "no:  1056230\n"
     ]
    }
   ],
   "source": [
    "scana = pre_processing(scana)\n",
    "scansu =  pre_processing(scansu)\n",
    "mqtt_bruteforce = pre_processing(mqtt_bruteforce)\n",
    "sparta = pre_processing(sparta)\n",
    "normal = pre_processing(normal)\n",
    "\n",
    "data = balance_and_concat(scana, scansu,mqtt_bruteforce,sparta,normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "jeN8faF4EuJB"
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "X, y = data.drop(columns=[\"protocol\",\"is_attack\"]).values, data['is_attack'].values\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "#print('X shape: ', X.shape)\n",
    "#print('X columns: ', len(data.columns))\n",
    "#print('X_new shape: ', X_new.shape)\n",
    "#print('feature importances: ', importances)\n",
    "\n",
    "selected_indexes = [i+1 for i in range(len(importances)) if importances[i] >= 0.1]\n",
    "selected_indexes = selected_indexes+[0,len(data.columns)-1]\n",
    "\n",
    "not_sel_col = [data.columns[i] for i in range(len(data.columns)) if i not in selected_indexes]\n",
    "sel_col = [data.columns[i] for i in range(len(data.columns)) if i in selected_indexes]\n",
    "\n",
    "data = data.drop(columns=not_sel_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "d2Sx-LiiqBxI"
   },
   "outputs": [],
   "source": [
    "#data = pd.concat([scana, scansu],axis=0)\n",
    "#data = pd.concat([scana, scansu,mqtt_bruteforce,normal],axis=0)\n",
    "\n",
    "#data = data.drop(columns=[\"timestamp\",\"src_ip\",\"dst_ip\", \"src_port\",\"dst_port\"])\n",
    "#data['protocol'] = data['protocol'].astype(str)\n",
    "#data['is_attack'] = data['is_attack'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "YnFr4wB0R16W",
    "outputId": "11024737-5504-4df5-cf6e-b0cfaba40b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "yes    1042401\n",
       "no     1042401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBpAtoWNQVS3",
    "outputId": "cd20630e-d5a8-4364-d7dd-659181b3ce8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "yes    1042401\n",
       "no     1042401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "GUdlv66QPZBI"
   },
   "outputs": [],
   "source": [
    "non_numeric_columns = data.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kI8XJxDcQ3k-",
    "outputId": "48444e55-753f-47b9-a003-4d2a5ccac54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['protocol', 'is_attack'], dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "8rQdXvYlSCQV"
   },
   "outputs": [],
   "source": [
    "#for col in data.columns:\n",
    "#    if col not in ['protocol', 'is_attack']:\n",
    "#      data[col] = pd.to_numeric(data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BQM_y5RtOZA",
    "outputId": "74ee217c-a7fa-4d8c-c39b-962030655510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "protocol               object\n",
       "ttl                   float64\n",
       "ip_len                float64\n",
       "tcp_flag_push         float64\n",
       "mqtt_messagetype      float64\n",
       "mqtt_messagelength    float64\n",
       "is_attack              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "ChkQXZuEOqMx",
    "outputId": "0145a7f5-00fa-48d7-dcc1-8c563a46ed4c"
   },
   "outputs": [],
   "source": [
    "#for column in data:\n",
    "#    if column not in ['protocol', 'is_attack']:\n",
    "#        clean_col = data[column].dropna()\n",
    "#        mean = clean_col.mean()\n",
    "#        data[column].fillna(mean, inplace=True)\n",
    "#\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "#\n",
    "#data['is_attack'].replace('0', 'no', inplace=True)\n",
    "#data['is_attack'].replace('1', 'yes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ENKR7UsCnDMk"
   },
   "outputs": [],
   "source": [
    "#ADD WEIGHTS - 1 FOR ALL SAMPLES\n",
    "data['fnlwgt'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12y9XwPHTK0U",
    "outputId": "e65d1f71-55d0-4283-e7e4-3b24ec0929e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "yes    1042401\n",
       "no     1042401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "2uuULEn1wmdt"
   },
   "outputs": [],
   "source": [
    "# Replace 'is_attack' with 'yes' only in rows where the value of 'is_attack' column is 'is_attack'\n",
    "#data.loc[data['is_attack'] == 'is_attack', 'is_attack'] = 'yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iU-HJfzyxw_c",
    "outputId": "ca5c4e13-1adb-4a93-aea1-82e28ce4d8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_attack\n",
       "yes    1042401\n",
       "no     1042401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN9cnu-InDPH",
    "outputId": "052cd9c1-e396-4e10-a449-d23d39fbfe5f"
   },
   "outputs": [],
   "source": [
    "##BALANCE DATASET\n",
    "#\n",
    "#aux = data.groupby('is_attack')\n",
    "#data = aux.apply(lambda x: x.sample(aux.size().min()).reset_index(drop=True))\n",
    "#\n",
    "#data = data.sample(frac=1).reset_index(drop=True)\n",
    "#print('quantity of no: ', data['is_attack'].value_counts()[0])\n",
    "#print('quantity of yes: ', data['is_attack'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "WZMjON4LnDRu"
   },
   "outputs": [],
   "source": [
    "data_train = data.loc[:int(0.8*len(data))]\n",
    "data_test = data.loc[int(0.8*len(data)):(int(0.8*len(data))+int(0.1*len(data)))]\n",
    "data_valid = data.loc[(int(0.8*len(data))+int(0.1*len(data))):]\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "valid_data_file = \"val_data.csv\"\n",
    "\n",
    "data_train.to_csv(train_data_file, index=False, header=False)\n",
    "data_test.to_csv(test_data_file, index=False, header=False)\n",
    "data_valid.to_csv(valid_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "opxG8tU_Dkq9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "73OcOd9wYJUg",
    "outputId": "30cfb420-5275-43b8-c6fe-86d8d3b889dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 3541\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Start training the model...\n",
      "Epoch 1/20\n",
      "   6294/Unknown \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.7002 - auc: 0.8001 - loss: 0.4831 - precision: 0.6744 - recall: 0.7691"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 02:59:58.733318: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 02:59:58.733340: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 02:59:58.733348: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 02:59:58.733353: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 02:59:58.733362: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "/home/irciss/anaconda3/envs/py39/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.7002 - auc: 0.8001 - loss: 0.4831 - precision: 0.6744 - recall: 0.7691 - val_accuracy: 0.7631 - val_auc: 0.8689 - val_loss: 0.3742 - val_precision: 0.6788 - val_recall: 0.9978\n",
      "Epoch 2/20\n",
      "\u001b[1m 124/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7548 - auc: 0.8617 - loss: 0.3841 - precision: 0.7003 - recall: 0.8868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 02:59:59.961406: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 02:59:59.961423: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 02:59:59.961426: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 02:59:59.961432: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 02:59:59.961434: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 02:59:59.961436: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 02:59:59.961438: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 02:59:59.961443: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6267/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7566 - auc: 0.8629 - loss: 0.3821 - precision: 0.7000 - recall: 0.8982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:07.606376: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7566 - auc: 0.8629 - loss: 0.3821 - precision: 0.7000 - recall: 0.8982 - val_accuracy: 0.7633 - val_auc: 0.8720 - val_loss: 0.3724 - val_precision: 0.6789 - val_recall: 0.9978\n",
      "Epoch 3/20\n",
      "\u001b[1m6273/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7592 - auc: 0.8644 - loss: 0.3786 - precision: 0.6958 - recall: 0.9214"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:15.788785: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:00:15.788799: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:00:15.788803: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:00:15.788805: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:00:15.788809: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7592 - auc: 0.8644 - loss: 0.3786 - precision: 0.6958 - recall: 0.9214 - val_accuracy: 0.7633 - val_auc: 0.8706 - val_loss: 0.3724 - val_precision: 0.6789 - val_recall: 0.9978\n",
      "Epoch 4/20\n",
      "\u001b[1m 126/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7573 - auc: 0.8642 - loss: 0.3785 - precision: 0.6920 - recall: 0.9233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:16.234010: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:00:16.234027: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:00:16.234032: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6252/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7604 - auc: 0.8651 - loss: 0.3768 - precision: 0.6937 - recall: 0.9327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:23.886510: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:00:23.886526: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:00:23.886530: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:00:23.886539: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7604 - auc: 0.8652 - loss: 0.3768 - precision: 0.6937 - recall: 0.9327 - val_accuracy: 0.7556 - val_auc: 0.8727 - val_loss: 0.3722 - val_precision: 0.7449 - val_recall: 0.7764\n",
      "Epoch 5/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7613 - auc: 0.8657 - loss: 0.3759 - precision: 0.6928 - recall: 0.9391 - val_accuracy: 0.7627 - val_auc: 0.8706 - val_loss: 0.3724 - val_precision: 0.7483 - val_recall: 0.7905\n",
      "Epoch 6/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7619 - auc: 0.8664 - loss: 0.3753 - precision: 0.6924 - recall: 0.9428 - val_accuracy: 0.7634 - val_auc: 0.8707 - val_loss: 0.3715 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 7/20\n",
      "\u001b[1m 122/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7593 - auc: 0.8676 - loss: 0.3756 - precision: 0.6881 - recall: 0.9440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:40.570094: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:00:40.570109: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:00:40.570114: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6275/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7618 - auc: 0.8668 - loss: 0.3748 - precision: 0.6914 - recall: 0.9461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:00:48.242420: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:00:48.242443: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7618 - auc: 0.8668 - loss: 0.3748 - precision: 0.6914 - recall: 0.9461 - val_accuracy: 0.7633 - val_auc: 0.8750 - val_loss: 0.3765 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 8/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7624 - auc: 0.8667 - loss: 0.3744 - precision: 0.6912 - recall: 0.9487 - val_accuracy: 0.7627 - val_auc: 0.8733 - val_loss: 0.3718 - val_precision: 0.7483 - val_recall: 0.7905\n",
      "Epoch 9/20\n",
      "\u001b[1m6263/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7623 - auc: 0.8671 - loss: 0.3741 - precision: 0.6912 - recall: 0.9485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:04.749137: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:01:04.749154: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:04.749158: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:01:04.749160: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:01:04.749162: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n",
      "2024-02-03 03:01:04.749166: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7623 - auc: 0.8671 - loss: 0.3741 - precision: 0.6912 - recall: 0.9485 - val_accuracy: 0.7634 - val_auc: 0.8733 - val_loss: 0.3712 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 10/20\n",
      "\u001b[1m6260/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7626 - auc: 0.8674 - loss: 0.3739 - precision: 0.6915 - recall: 0.9488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:12.836699: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:01:12.836721: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:12.836728: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7626 - auc: 0.8674 - loss: 0.3739 - precision: 0.6915 - recall: 0.9487 - val_accuracy: 0.7634 - val_auc: 0.8722 - val_loss: 0.3715 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 11/20\n",
      "\u001b[1m 118/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7613 - auc: 0.8684 - loss: 0.3748 - precision: 0.6882 - recall: 0.9512"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:13.285735: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:13.285751: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:01:13.285754: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:01:13.285758: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6267/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7626 - auc: 0.8680 - loss: 0.3736 - precision: 0.6914 - recall: 0.9486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:21.027514: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:01:21.027533: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:21.027540: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7626 - auc: 0.8680 - loss: 0.3736 - precision: 0.6914 - recall: 0.9486 - val_accuracy: 0.7556 - val_auc: 0.8726 - val_loss: 0.3717 - val_precision: 0.7449 - val_recall: 0.7764\n",
      "Epoch 12/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7630 - auc: 0.8681 - loss: 0.3734 - precision: 0.6917 - recall: 0.9496 - val_accuracy: 0.7634 - val_auc: 0.8734 - val_loss: 0.3711 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 13/20\n",
      "\u001b[1m6280/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7631 - auc: 0.8684 - loss: 0.3732 - precision: 0.6917 - recall: 0.9497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:37.444965: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:01:37.444987: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:37.444994: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:01:37.445000: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:01:37.445008: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7631 - auc: 0.8684 - loss: 0.3732 - precision: 0.6917 - recall: 0.9497 - val_accuracy: 0.7600 - val_auc: 0.8734 - val_loss: 0.3719 - val_precision: 0.7471 - val_recall: 0.7852\n",
      "Epoch 14/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7626 - auc: 0.8679 - loss: 0.3732 - precision: 0.6910 - recall: 0.9505 - val_accuracy: 0.7634 - val_auc: 0.8707 - val_loss: 0.3711 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 15/20\n",
      "\u001b[1m6291/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7629 - auc: 0.8680 - loss: 0.3731 - precision: 0.6902 - recall: 0.9543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:01:53.764260: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:01:53.764276: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:01:53.764279: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:01:53.764283: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:01:53.764284: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:01:53.764286: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n",
      "2024-02-03 03:01:53.764288: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 03:01:53.764293: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7629 - auc: 0.8680 - loss: 0.3731 - precision: 0.6902 - recall: 0.9543 - val_accuracy: 0.7634 - val_auc: 0.8729 - val_loss: 0.3710 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 16/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7634 - auc: 0.8686 - loss: 0.3727 - precision: 0.6898 - recall: 0.9577 - val_accuracy: 0.7634 - val_auc: 0.8734 - val_loss: 0.3710 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 17/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7635 - auc: 0.8686 - loss: 0.3727 - precision: 0.6898 - recall: 0.9580 - val_accuracy: 0.7627 - val_auc: 0.8729 - val_loss: 0.3713 - val_precision: 0.7483 - val_recall: 0.7905\n",
      "Epoch 18/20\n",
      "\u001b[1m6263/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7638 - auc: 0.8689 - loss: 0.3724 - precision: 0.6895 - recall: 0.9602"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:02:18.311661: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7638 - auc: 0.8689 - loss: 0.3724 - precision: 0.6895 - recall: 0.9602 - val_accuracy: 0.7634 - val_auc: 0.8725 - val_loss: 0.3710 - val_precision: 0.6790 - val_recall: 0.9978\n",
      "Epoch 19/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7638 - auc: 0.8694 - loss: 0.3722 - precision: 0.6896 - recall: 0.9597 - val_accuracy: 0.7556 - val_auc: 0.8735 - val_loss: 0.3717 - val_precision: 0.7449 - val_recall: 0.7764\n",
      "Epoch 20/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.7637 - auc: 0.8694 - loss: 0.3721 - precision: 0.6892 - recall: 0.9612 - val_accuracy: 0.7634 - val_auc: 0.8739 - val_loss: 0.3710 - val_precision: 0.6790 - val_recall: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:02:35.021639: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:02:35.021656: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:02:35.021658: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:02:35.021662: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 171.04 seconds\n",
      "Model training finished\n",
      "Validation accuracy: 76.34%\n",
      "Inference time: 0.46 seconds\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 749us/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.69    104385\n",
      "           1       0.68      1.00      0.81    104096\n",
      "\n",
      "    accuracy                           0.76    208481\n",
      "   macro avg       0.84      0.76      0.75    208481\n",
      "weighted avg       0.84      0.76      0.75    208481\n",
      "\n",
      "AUC ROC Score: 0.7637033727353579\n",
      "Total model weights: 21231\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Start training the model...\n",
      "Epoch 1/20\n",
      "   6294/Unknown \u001b[1m39s\u001b[0m 4ms/step - accuracy: 0.7404 - auc: 0.8537 - loss: 0.4112 - precision: 0.7129 - recall: 0.8050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irciss/anaconda3/envs/py39/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.7404 - auc: 0.8537 - loss: 0.4112 - precision: 0.7129 - recall: 0.8050 - val_accuracy: 0.7614 - val_auc: 0.8733 - val_loss: 0.3817 - val_precision: 0.6772 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m  48/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7566 - auc: 0.8620 - loss: 0.3866 - precision: 0.7036 - recall: 0.8720"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:03:18.003558: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:03:18.003581: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:03:18.003588: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:03:18.003594: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:03:18.003599: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:03:18.003604: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n",
      "2024-02-03 03:03:18.003608: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:03:18.003617: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7536 - auc: 0.8635 - loss: 0.3871 - precision: 0.7062 - recall: 0.8688 - val_accuracy: 0.7614 - val_auc: 0.8708 - val_loss: 0.3801 - val_precision: 0.6772 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7562 - auc: 0.8642 - loss: 0.3815 - precision: 0.6929 - recall: 0.9059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:03:37.556828: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:03:37.556851: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:03:37.556858: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:03:37.556863: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:03:37.556871: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6291/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7557 - auc: 0.8642 - loss: 0.3831 - precision: 0.7015 - recall: 0.8905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:03:56.129909: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:03:56.129925: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:03:56.129928: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:03:56.129931: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 03:03:56.129933: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:03:56.129935: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:03:56.129937: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:03:56.129942: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7557 - auc: 0.8642 - loss: 0.3831 - precision: 0.7015 - recall: 0.8905 - val_accuracy: 0.7615 - val_auc: 0.8711 - val_loss: 0.3770 - val_precision: 0.6772 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7561 - auc: 0.8647 - loss: 0.3812 - precision: 0.7001 - recall: 0.8963 - val_accuracy: 0.7615 - val_auc: 0.8711 - val_loss: 0.3770 - val_precision: 0.6773 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m  59/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.7540 - auc: 0.8660 - loss: 0.3787 - precision: 0.6913 - recall: 0.9037"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:04:15.503433: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:04:15.503451: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:04:15.503455: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7566 - auc: 0.8655 - loss: 0.3795 - precision: 0.6997 - recall: 0.8994 - val_accuracy: 0.7615 - val_auc: 0.8734 - val_loss: 0.3746 - val_precision: 0.6773 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m  51/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7533 - auc: 0.8632 - loss: 0.3793 - precision: 0.6863 - recall: 0.9178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:04:34.043186: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:04:34.043204: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7582 - auc: 0.8660 - loss: 0.3781 - precision: 0.6991 - recall: 0.9071 - val_accuracy: 0.7615 - val_auc: 0.8741 - val_loss: 0.3758 - val_precision: 0.6773 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m  58/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.7589 - auc: 0.8671 - loss: 0.3762 - precision: 0.6871 - recall: 0.9365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:04:53.221141: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:04:53.221166: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:04:53.221174: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:04:53.221180: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:04:53.221189: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6289/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7585 - auc: 0.8667 - loss: 0.3770 - precision: 0.6978 - recall: 0.9121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:11.587889: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:05:11.587906: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:05:11.587909: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:05:11.587912: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 03:05:11.587914: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:05:11.587916: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:05:11.587919: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:05:11.587925: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7585 - auc: 0.8667 - loss: 0.3770 - precision: 0.6978 - recall: 0.9121 - val_accuracy: 0.7628 - val_auc: 0.8744 - val_loss: 0.3735 - val_precision: 0.6785 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m  54/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7596 - auc: 0.8666 - loss: 0.3754 - precision: 0.6881 - recall: 0.9347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:12.281826: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:05:12.281842: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:05:12.281844: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6285/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7591 - auc: 0.8669 - loss: 0.3765 - precision: 0.6975 - recall: 0.9153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:30.995811: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7591 - auc: 0.8669 - loss: 0.3765 - precision: 0.6975 - recall: 0.9153 - val_accuracy: 0.7628 - val_auc: 0.8755 - val_loss: 0.3762 - val_precision: 0.6785 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m  50/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7566 - auc: 0.8680 - loss: 0.3761 - precision: 0.6860 - recall: 0.9308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:31.756506: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6292/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7596 - auc: 0.8673 - loss: 0.3761 - precision: 0.6974 - recall: 0.9172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:50.826582: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:05:50.826605: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7596 - auc: 0.8673 - loss: 0.3761 - precision: 0.6974 - recall: 0.9172 - val_accuracy: 0.7630 - val_auc: 0.8744 - val_loss: 0.3742 - val_precision: 0.6787 - val_recall: 0.9978 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m  52/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7600 - auc: 0.8663 - loss: 0.3768 - precision: 0.6790 - recall: 0.9697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:05:51.528452: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:05:51.528467: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:05:51.528471: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:05:51.528474: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:05:51.528478: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7614 - auc: 0.8687 - loss: 0.3747 - precision: 0.6950 - recall: 0.9320 - val_accuracy: 0.7632 - val_auc: 0.8737 - val_loss: 0.3723 - val_precision: 0.6789 - val_recall: 0.9978 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m  56/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7616 - auc: 0.8679 - loss: 0.3745 - precision: 0.6871 - recall: 0.9457"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:06:10.253436: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:06:10.253455: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:06:10.253460: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7618 - auc: 0.8688 - loss: 0.3745 - precision: 0.6949 - recall: 0.9338 - val_accuracy: 0.7631 - val_auc: 0.8737 - val_loss: 0.3722 - val_precision: 0.6788 - val_recall: 0.9978 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m  49/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7646 - auc: 0.8713 - loss: 0.3737 - precision: 0.6907 - recall: 0.9419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:06:28.886024: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:06:28.886044: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6283/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7619 - auc: 0.8688 - loss: 0.3744 - precision: 0.6948 - recall: 0.9344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:06:46.948924: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:06:46.948948: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:06:46.948955: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:06:46.948961: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 03:06:46.948966: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:06:46.948977: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7619 - auc: 0.8688 - loss: 0.3744 - precision: 0.6948 - recall: 0.9344 - val_accuracy: 0.7631 - val_auc: 0.8737 - val_loss: 0.3722 - val_precision: 0.6788 - val_recall: 0.9978 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.7620 - auc: 0.8690 - loss: 0.3742 - precision: 0.6945 - recall: 0.9359 - val_accuracy: 0.7631 - val_auc: 0.8737 - val_loss: 0.3722 - val_precision: 0.6788 - val_recall: 0.9978 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m  55/6294\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.7587 - auc: 0.8667 - loss: 0.3749 - precision: 0.6831 - recall: 0.9495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:07:07.314607: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:07:07.314623: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:07:07.314626: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:07:07.314629: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:07:07.314631: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:07:07.314633: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n",
      "2024-02-03 03:07:07.314635: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:07:07.314639: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6285/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7621 - auc: 0.8691 - loss: 0.3741 - precision: 0.6941 - recall: 0.9374"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:07:24.981793: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9268208623841757565\n",
      "2024-02-03 03:07:24.981813: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15520934327594066847\n",
      "2024-02-03 03:07:24.981816: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:07:24.981819: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10139441375753848656\n",
      "2024-02-03 03:07:24.981821: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n",
      "2024-02-03 03:07:24.981823: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9289195203008978844\n",
      "2024-02-03 03:07:24.981825: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2198605829791602770\n",
      "2024-02-03 03:07:24.981830: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1941625320854460230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.7621 - auc: 0.8691 - loss: 0.3741 - precision: 0.6941 - recall: 0.9374 - val_accuracy: 0.7631 - val_auc: 0.8737 - val_loss: 0.3722 - val_precision: 0.6788 - val_recall: 0.9978 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m6294/6294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.7622 - auc: 0.8692 - loss: 0.3741 - precision: 0.6942 - recall: 0.9374 - val_accuracy: 0.7631 - val_auc: 0.8737 - val_loss: 0.3722 - val_precision: 0.6788 - val_recall: 0.9978 - learning_rate: 1.0000e-06\n",
      "Training finished in 308.36 seconds\n",
      "Model training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 03:07:45.814288: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4592894476644585425\n",
      "2024-02-03 03:07:45.814309: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4448486198308456762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 76.31%\n",
      "Inference time: 0.72 seconds\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.69    104385\n",
      "           1       0.68      1.00      0.81    104096\n",
      "\n",
      "    accuracy                           0.76    208481\n",
      "   macro avg       0.84      0.76      0.75    208481\n",
      "weighted avg       0.84      0.76      0.75    208481\n",
      "\n",
      "AUC ROC Score: 0.7634734413453907\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "## Prepare the data\n",
    "\n",
    "data_columns = list(data.columns)\n",
    "\n",
    "CSV_HEADER = data_columns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = data_columns[1:len(data_columns)-2]\n",
    "\n",
    "\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"protocol\": sorted(list(data_train[\"protocol\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"is_attack\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"yes\", \"no\"]\n",
    "\n",
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 4  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "lookup_dict = {}\n",
    "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "    # Create a lookup to convert a string values to an integer indices.\n",
    "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "    lookup = layers.StringLookup(\n",
    "        vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "    )\n",
    "    lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "def encode_categorical(batch_x, batch_y, weights):\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name=TARGET_FEATURE_NAME,\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "        .map(encode_categorical)\n",
    "    )\n",
    "    return dataset.cache()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "    callbacks=None,\n",
    "):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "                ],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset,callbacks=callbacks\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training finished in {training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Model training finished\")\n",
    "    start_time = time.time()\n",
    "    loss, accuracy, precision, recall, auc  = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "    # Assuming your model's final layer is a sigmoid, as you're using BinaryCrossentropy\n",
    "    # If it's softmax, you'll need to adapt this and use `predict_classes` or similar\n",
    "    y_pred = (model.predict(validation_dataset) > 0.5).astype(\"int32\")\n",
    "    y_true = []\n",
    "    for x, y, w in validation_dataset:\n",
    "        y_true.extend(y.numpy())\n",
    "    y_true = np.array(y_true).flatten()  # Ensure y_true is a flat array\n",
    "    assert len(y_true) == len(y_pred), \"Mismatch in the number of samples between y_true and y_pred\"\n",
    "\n",
    "\n",
    "    #print(f\"Validation accuracy: {results[1]:.2f}%\")\n",
    "    #print(f\"AUC: {results[-1]}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Assuming binary classification and y_true and y_pred are numpy arrays\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"AUC ROC Score: {auc_score}\")\n",
    "\n",
    "    return history, training_time, inference_time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create model inputs\n",
    "\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer()),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Experiment 1: a baseline model\n",
    "\n",
    "In the first experiment, we create a simple multi-layer feed-forward network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization,\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization,\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the baseline model:\n",
    "\"\"\"\n",
    "\n",
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "## Experiment 2: TabTransformer\n",
    "\n",
    "\n",
    "\n",
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "\n",
    "\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=partial(\n",
    "                layers.LayerNormalization, epsilon=1e-6\n",
    "            ),  # using partial to provide keyword arguments before initialization\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Batch Normalization before the MultiHead Attention\n",
    "        encoded_categorical_features = layers.BatchNormalization()(encoded_categorical_features)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization,\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "# Define callbacks for learning rate scheduling and early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
    "\n",
    "# ...\n",
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping_callback, reduce_lr_callback]  # Add callbacks here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "IGvFYQs8Dkq_",
    "outputId": "60a7aded-919e-49e5-ec52-3b5a6c5cc507"
   },
   "outputs": [],
   "source": [
    "# def plot_learning_curves(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "\n",
    "#     epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#     plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#     plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# history, training_time, inference_time = run_experiment(\n",
    "#         model=tabtransformer_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[early_stopping_callback, reduce_lr_callback]\n",
    "# )\n",
    "\n",
    "# plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "m5B3AZ-9DkrB"
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# ## Prepare the data\n",
    "\n",
    "# data_columns = list(data.columns)\n",
    "\n",
    "# CSV_HEADER = data_columns\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Define dataset metadata\n",
    "\n",
    "# Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "# the data into input features, and encoding the input features with respect to their types.\n",
    "# \"\"\"\n",
    "\n",
    "# # A list of the numerical feature names.\n",
    "# NUMERIC_FEATURE_NAMES = data_columns[1:len(data_columns)-2]\n",
    "\n",
    "\n",
    "# # A dictionary of the categorical features and their vocabulary.\n",
    "# CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "#     \"protocol\": sorted(list(data_train[\"protocol\"].unique())),\n",
    "# }\n",
    "# # Name of the column to be used as instances weight.\n",
    "# WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# # A list of the categorical feature names.\n",
    "# CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# # A list of all the input features.\n",
    "# FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# # A list of column default values for each feature.\n",
    "# COLUMN_DEFAULTS = [\n",
    "#     [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "#     for feature_name in CSV_HEADER\n",
    "# ]\n",
    "# # The name of the target feature.\n",
    "# TARGET_FEATURE_NAME = \"is_attack\"\n",
    "# # A list of the labels of the target features.\n",
    "# TARGET_LABELS = [\"yes\", \"no\"]\n",
    "\n",
    "# \"\"\"\n",
    "# ## Configure the hyperparameters\n",
    "\n",
    "# The hyperparameters includes model architecture and training configurations.\n",
    "# \"\"\"\n",
    "\n",
    "# LEARNING_RATE = 0.001\n",
    "# WEIGHT_DECAY = 0.0001\n",
    "# DROPOUT_RATE = 0.2\n",
    "# BATCH_SIZE = 265\n",
    "# NUM_EPOCHS = 20\n",
    "\n",
    "# NUM_TRANSFORMER_BLOCKS = 4  # Number of transformer blocks.\n",
    "# NUM_HEADS = 4  # Number of attention heads.\n",
    "# EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "# MLP_HIDDEN_UNITS_FACTORS = [\n",
    "#     2,\n",
    "#     1,\n",
    "# ]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "# NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement data reading pipeline\n",
    "\n",
    "# We define an input function that reads and parses the file, then converts features\n",
    "# and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "# for training or evaluation.\n",
    "# \"\"\"\n",
    "\n",
    "# target_label_lookup = layers.StringLookup(\n",
    "#     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    "# )\n",
    "\n",
    "\n",
    "# def prepare_example(features, target):\n",
    "#     target_index = target_label_lookup(target)\n",
    "#     weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "#     return features, target_index, weights\n",
    "\n",
    "\n",
    "# lookup_dict = {}\n",
    "# for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#     vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "#     # Create a lookup to convert a string values to an integer indices.\n",
    "#     # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "#     # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "#     lookup = layers.StringLookup(\n",
    "#         vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "#     )\n",
    "#     lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "# def encode_categorical(batch_x, batch_y, weights):\n",
    "#     for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#         batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "#     return batch_x, batch_y, weights\n",
    "\n",
    "\n",
    "# def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "#     dataset = (\n",
    "#         tf_data.experimental.make_csv_dataset(\n",
    "#             csv_file_path,\n",
    "#             batch_size=batch_size,\n",
    "#             column_names=CSV_HEADER,\n",
    "#             column_defaults=COLUMN_DEFAULTS,\n",
    "#             label_name=TARGET_FEATURE_NAME,\n",
    "#             num_epochs=1,\n",
    "#             header=False,\n",
    "#             na_value=\"?\",\n",
    "#             shuffle=shuffle,\n",
    "#         )\n",
    "#         .map(prepare_example, num_parallel_calls=tf_data.AUTOTUNE, deterministic=False)\n",
    "#         .map(encode_categorical)\n",
    "#     )\n",
    "#     return dataset.cache()\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement a training and evaluation procedure\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def run_experiment(\n",
    "#     model,\n",
    "#     train_data_file,\n",
    "#     test_data_file,\n",
    "#     num_epochs,\n",
    "#     learning_rate,\n",
    "#     weight_decay,\n",
    "#     batch_size,\n",
    "#     callbacks=None,\n",
    "# ):\n",
    "#     optimizer = keras.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.BinaryCrossentropy(),\n",
    "#         metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "#     )\n",
    "\n",
    "#     train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "#     validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "#     print(\"Start training the model...\")\n",
    "#     history = model.fit(\n",
    "#         train_dataset, epochs=num_epochs, validation_data=validation_dataset,callbacks=callbacks\n",
    "#     )\n",
    "#     print(\"Model training finished\")\n",
    "\n",
    "#     _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "#     print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "#     return history\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Create model inputs\n",
    "\n",
    "# Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "# and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "# and data type.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_model_inputs():\n",
    "#     inputs = {}\n",
    "#     for feature_name in FEATURE_NAMES:\n",
    "#         if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "#             inputs[feature_name] = layers.Input(\n",
    "#                 name=feature_name, shape=(), dtype=\"float32\"\n",
    "#             )\n",
    "#         else:\n",
    "#             inputs[feature_name] = layers.Input(\n",
    "#                 name=feature_name, shape=(), dtype=\"float32\"\n",
    "#             )\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Encode features\n",
    "\n",
    "# The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "# We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "# regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def encode_inputs(inputs, embedding_dims):\n",
    "#     encoded_categorical_feature_list = []\n",
    "#     numerical_feature_list = []\n",
    "\n",
    "#     for feature_name in inputs:\n",
    "#         if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "#             vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "#             # Create a lookup to convert a string values to an integer indices.\n",
    "#             # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "#             # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "\n",
    "#             # Convert the string input values into integer indices.\n",
    "\n",
    "#             # Create an embedding layer with the specified dimensions.\n",
    "#             embedding = layers.Embedding(\n",
    "#                 input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "#             )\n",
    "\n",
    "#             # Convert the index values to embedding representations.\n",
    "#             encoded_categorical_feature = embedding(inputs[feature_name])\n",
    "#             encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "#         else:\n",
    "#             # Use the numerical features as-is.\n",
    "#             numerical_feature = ops.expand_dims(inputs[feature_name], -1)\n",
    "#             numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "#     return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Implement an MLP block\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "#     mlp_layers = []\n",
    "#     for units in hidden_units:\n",
    "#         mlp_layers.append(normalization_layer()),\n",
    "#         mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "#         mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "#     return keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# ## Experiment 1: a baseline model\n",
    "\n",
    "# In the first experiment, we create a simple multi-layer feed-forward network.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def create_baseline_model(\n",
    "#     embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "# ):\n",
    "#     # Create model inputs.\n",
    "#     inputs = create_model_inputs()\n",
    "#     # encode features.\n",
    "#     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "#         inputs, embedding_dims\n",
    "#     )\n",
    "#     # Concatenate all features.\n",
    "#     features = layers.concatenate(\n",
    "#         encoded_categorical_feature_list + numerical_feature_list\n",
    "#     )\n",
    "#     # Compute Feedforward layer units.\n",
    "#     feedforward_units = [features.shape[-1]]\n",
    "\n",
    "#     # Create several feedforwad layers with skip connections.\n",
    "#     for layer_idx in range(num_mlp_blocks):\n",
    "#         features = create_mlp(\n",
    "#             hidden_units=feedforward_units,\n",
    "#             dropout_rate=dropout_rate,\n",
    "#             activation=keras.activations.gelu,\n",
    "#             normalization_layer=layers.LayerNormalization,\n",
    "#             name=f\"feedforward_{layer_idx}\",\n",
    "#         )(features)\n",
    "\n",
    "#     # Compute MLP hidden_units.\n",
    "#     mlp_hidden_units = [\n",
    "#         factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "#     ]\n",
    "#     # Create final MLP.\n",
    "#     features = create_mlp(\n",
    "#         hidden_units=mlp_hidden_units,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         activation=keras.activations.selu,\n",
    "#         normalization_layer=layers.BatchNormalization,\n",
    "#         name=\"MLP\",\n",
    "#     )(features)\n",
    "\n",
    "#     # Add a sigmoid as a binary classifer.\n",
    "#     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# baseline_model = create_baseline_model(\n",
    "#     embedding_dims=EMBEDDING_DIMS,\n",
    "#     num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "#     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# print(\"Total model weights:\", baseline_model.count_params())\n",
    "# keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "# \"\"\"\n",
    "# Let's train and evaluate the baseline model:\n",
    "# \"\"\"\n",
    "\n",
    "# history = run_experiment(\n",
    "#     model=baseline_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# ## Experiment 2: TabTransformer\n",
    "\n",
    "\n",
    "\n",
    "# def create_tabtransformer_classifier(\n",
    "#     num_transformer_blocks,\n",
    "#     num_heads,\n",
    "#     embedding_dims,\n",
    "#     mlp_hidden_units_factors,\n",
    "#     dropout_rate,\n",
    "#     use_column_embedding=False,\n",
    "# ):\n",
    "#     # Create model inputs.\n",
    "#     inputs = create_model_inputs()\n",
    "#     # encode features.\n",
    "#     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "#         inputs, embedding_dims\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # Stack categorical feature embeddings for the Tansformer.\n",
    "#     encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)\n",
    "#     # Concatenate numerical features.\n",
    "#     numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "#     # Add column embedding to categorical feature embeddings.\n",
    "#     if use_column_embedding:\n",
    "#         num_columns = encoded_categorical_features.shape[1]\n",
    "#         column_embedding = layers.Embedding(\n",
    "#             input_dim=num_columns, output_dim=embedding_dims\n",
    "#         )\n",
    "#         column_indices = ops.arange(start=0, stop=num_columns, step=1)\n",
    "#         encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "#             column_indices\n",
    "#         )\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for block_idx in range(num_transformer_blocks):\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads,\n",
    "#             key_dim=embedding_dims,\n",
    "#             dropout=dropout_rate,\n",
    "#             name=f\"multihead_attention_{block_idx}\",\n",
    "#         )(encoded_categorical_features, encoded_categorical_features)\n",
    "#         # Skip connection 1.\n",
    "#         x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "#             [attention_output, encoded_categorical_features]\n",
    "#         )\n",
    "#         # Layer normalization 1.\n",
    "#         x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "#         # Feedforward.\n",
    "#         feedforward_output = create_mlp(\n",
    "#             hidden_units=[embedding_dims],\n",
    "#             dropout_rate=dropout_rate,\n",
    "#             activation=keras.activations.gelu,\n",
    "#             normalization_layer=partial(\n",
    "#                 layers.LayerNormalization, epsilon=1e-6\n",
    "#             ),  # using partial to provide keyword arguments before initialization\n",
    "#             name=f\"feedforward_{block_idx}\",\n",
    "#         )(x)\n",
    "#         # Skip connection 2.\n",
    "#         x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "#         # Layer normalization 2.\n",
    "#         encoded_categorical_features = layers.LayerNormalization(\n",
    "#             name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "#         )(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "#     categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "#     # Apply layer normalization to the numerical features.\n",
    "#     numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "#     # Prepare the input for the final MLP block.\n",
    "#     features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for block_idx in range(num_transformer_blocks):\n",
    "#         # Batch Normalization before the MultiHead Attention\n",
    "#         encoded_categorical_features = layers.BatchNormalization()(encoded_categorical_features)\n",
    "\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads,\n",
    "#             key_dim=embedding_dims,\n",
    "#             dropout=dropout_rate,\n",
    "#             name=f\"multihead_attention_{block_idx}\",\n",
    "#         )(encoded_categorical_features, encoded_categorical_features)\n",
    "\n",
    "#         # Skip connection 1.\n",
    "#         x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "#             [attention_output, encoded_categorical_features]\n",
    "#         )\n",
    "#         # Layer normalization 1.\n",
    "#         x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Compute MLP hidden_units.\n",
    "#     mlp_hidden_units = [\n",
    "#         factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "#     ]\n",
    "#     # Create final MLP.\n",
    "#     features = create_mlp(\n",
    "#         hidden_units=mlp_hidden_units,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         activation=keras.activations.selu,\n",
    "#         normalization_layer=layers.BatchNormalization,\n",
    "#         name=\"MLP\",\n",
    "#     )(features)\n",
    "\n",
    "#     # Add a sigmoid as a binary classifer.\n",
    "#     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tabtransformer_model = create_tabtransformer_classifier(\n",
    "#     num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     embedding_dims=EMBEDDING_DIMS,\n",
    "#     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "# keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "# \"\"\"\n",
    "# Let's train and evaluate the TabTransformer model:\n",
    "# \"\"\"\n",
    "# # Define callbacks for learning rate scheduling and early stopping\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n",
    "\n",
    "# # ...\n",
    "# history = run_experiment(\n",
    "#     model=tabtransformer_model,\n",
    "#     train_data_file=train_data_file,\n",
    "#     test_data_file=test_data_file,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     weight_decay=WEIGHT_DECAY,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     callbacks=[early_stopping_callback, reduce_lr_callback]  # Add callbacks here\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7aKpTARDkrD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQvxY0FlDkrD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3157581,
     "sourceId": 5466512,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4137697,
     "sourceId": 7163258,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
